{
  
    
        "post0": {
            "title": "Notes on Natural Langage Processing with Deep Learning",
            "content": "Word to Vector for Oil and Gas .",
            "url": "https://marcelcastrobr.github.io/notebooks/2022/01/04/NLPDeepLearning.html",
            "relUrl": "/2022/01/04/NLPDeepLearning.html",
            "date": " • Jan 4, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Multilingual Joint Image & Text Embeddings",
            "content": ". %%capture !pip install sentence-transformers . from sentence_transformers import SentenceTransformer, util from PIL import Image import glob import torch import pickle import zipfile from IPython.display import display from IPython.display import Image as IPImage import os from tqdm.autonotebook import tqdm # Here we load the multilingual CLIP model. Note, this model can only encode text. # If you need embeddings for images, you must load the &#39;clip-ViT-B-32&#39; model model = SentenceTransformer(&#39;clip-ViT-B-32-multilingual-v1&#39;) . img_folder = &#39;photos/&#39; if not os.path.exists(img_folder) or len(os.listdir(img_folder)) == 0: os.makedirs(img_folder, exist_ok=True) photo_filename = &#39;unsplash-25k-photos.zip&#39; if not os.path.exists(photo_filename): #Download dataset if does not exist util.http_get(&#39;http://sbert.net/datasets/&#39;+photo_filename, photo_filename) #Extract all images with zipfile.ZipFile(photo_filename, &#39;r&#39;) as zf: for member in tqdm(zf.infolist(), desc=&#39;Extracting&#39;): zf.extract(member, img_folder) . # To speed things up, we destribute pre-computed embeddings # Otherwise you can also encode the images yourself. # To encode an image, you can use the following code: # from PIL import Image # img_emb = model.encode(Image.open(filepath)) use_precomputed_embeddings = True if use_precomputed_embeddings: emb_filename = &#39;unsplash-25k-photos-embeddings.pkl&#39; if not os.path.exists(emb_filename): #Download dataset if does not exist util.http_get(&#39;http://sbert.net/datasets/&#39;+emb_filename, emb_filename) with open(emb_filename, &#39;rb&#39;) as fIn: img_names, img_emb = pickle.load(fIn) print(&quot;Images:&quot;, len(img_names)) else: #For embedding images, we need the non-multilingual CLIP model img_model = SentenceTransformer(&#39;clip-ViT-B-32&#39;) img_names = list(glob.glob(&#39;photos/*.jpg&#39;)) print(&quot;Images:&quot;, len(img_names)) img_emb = img_model.encode([Image.open(filepath) for filepath in img_names], batch_size=128, convert_to_tensor=True, show_progress_bar=True) . Images: 24996 . import torch filepath = &#39;photos/&#39;+img_names[0] one_emb = torch.tensor(img_emb[0]) img_model = SentenceTransformer(&#39;clip-ViT-B-32&#39;) comb_emb = img_model.encode(Image.open(filepath), convert_to_tensor=True).cpu() print(util.cos_sim(one_emb, comb_emb)) . ftfy or spacy is not installed using BERT BasicTokenizer instead of ftfy. /usr/local/lib/python3.7/dist-packages/transformers/feature_extraction_utils.py:158: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:201.) tensor = as_tensor(value) . tensor([[1.0000]]) . def search(query, k=3): # First, we encode the query (which can either be an image or a text string) query_emb = model.encode([query], convert_to_tensor=True, show_progress_bar=False) # Then, we use the util.semantic_search function, which computes the cosine-similarity # between the query embedding and all image embeddings. # It then returns the top_k highest ranked images, which we output hits = util.semantic_search(query_emb, img_emb, top_k=k)[0] print(&quot;Query:&quot;) display(query) for hit in hits: print(img_names[hit[&#39;corpus_id&#39;]]) display(IPImage(os.path.join(img_folder, img_names[hit[&#39;corpus_id&#39;]]), width=200)) . search(&quot;Two dogs playing in the snow&quot;) . Query: . &#39;Two dogs playing in the snow&#39; . lyStEjlKNSw.jpg . FAcSe7SjDUU.jpg Hb6nGDgWztE.jpg . search(&quot;Eine Katze auf einem Stuhl&quot;) . Query: . &#39;Eine Katze auf einem Stuhl&#39; . CgGDzMYdYw8.jpg . kjERLXaHjXc.jpg . I-YJ-gaJNaw.jpg . search(&quot;Muchos peces&quot;) . Query: . &#39;Muchos peces&#39; . H22jcGTyrS4.jpg . CJ_9I6aXSnc.jpg . _MJKaRig1Ic.jpg . search(&quot;棕榈树的沙滩&quot;) . Query: . &#39;棕榈树的沙滩&#39; . crIXKhUDpBI.jpg . _6iV1AJZ53s.jpg . rv63du1a79E.jpg . search(&quot;Закат на пляже&quot;) . Query: . &#39;Закат на пляже&#39; . JC5U3Eyiyr4.jpg . 5z1QDcisnJ8.jpg . rdG4hRoyVR0.jpg . search(&quot;Parkta bir köpek&quot;) . Query: . &#39;Parkta bir köpek&#39; . ROJLfAbL1Ig.jpg . 0O9A0F_d1qA.jpg . 4mdsPUtN0P0.jpg . search(&quot;夜のニューヨーク&quot;) . Query: . &#39;夜のニューヨーク&#39; . FGjR4IGwP7U.jpg . 8nCMOFYyXF4.jpg . ZAOEjcpdMkc.jpg . search(&quot;Dois cachorro&quot;) . Query: . &#39;Dois cachorro&#39; . kFucQoKaQ3g.jpg . aPtPQFyLxMM.jpg . oAGoeMbr1-4.jpg .",
            "url": "https://marcelcastrobr.github.io/notebooks/2022/01/03/Image_Search_multilingual.html",
            "relUrl": "/2022/01/03/Image_Search_multilingual.html",
            "date": " • Jan 3, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Joint Image & Text Embeddings",
            "content": ". Installation . First we need to install sentence-transformers . !pip install sentence-transformers . Collecting sentence-transformers Downloading sentence-transformers-2.1.0.tar.gz (78 kB) |████████████████████████████████| 78 kB 3.4 MB/s Collecting transformers&lt;5.0.0,&gt;=4.6.0 Downloading transformers-4.12.3-py3-none-any.whl (3.1 MB) |████████████████████████████████| 3.1 MB 11.3 MB/s Collecting tokenizers&gt;=0.10.3 Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB) |████████████████████████████████| 3.3 MB 28.9 MB/s Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (4.62.3) Requirement already satisfied: torch&gt;=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.10.0+cu111) Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (0.11.1+cu111) Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.19.5) Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.0.1) Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (1.4.1) Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence-transformers) (3.2.5) Collecting sentencepiece Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB) |████████████████████████████████| 1.2 MB 34.2 MB/s Collecting huggingface-hub Downloading huggingface_hub-0.1.2-py3-none-any.whl (59 kB) |████████████████████████████████| 59 kB 6.1 MB/s Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch&gt;=1.6.0-&gt;sentence-transformers) (3.10.0.2) Collecting sacremoses Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB) |████████████████████████████████| 895 kB 38.8 MB/s Requirement already satisfied: packaging&gt;=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers&lt;5.0.0,&gt;=4.6.0-&gt;sentence-transformers) (21.2) Collecting pyyaml&gt;=5.1 Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB) |████████████████████████████████| 596 kB 42.8 MB/s Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers&lt;5.0.0,&gt;=4.6.0-&gt;sentence-transformers) (4.8.2) Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers&lt;5.0.0,&gt;=4.6.0-&gt;sentence-transformers) (2019.12.20) Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers&lt;5.0.0,&gt;=4.6.0-&gt;sentence-transformers) (2.23.0) Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers&lt;5.0.0,&gt;=4.6.0-&gt;sentence-transformers) (3.3.2) Requirement already satisfied: pyparsing&lt;3,&gt;=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging&gt;=20.0-&gt;transformers&lt;5.0.0,&gt;=4.6.0-&gt;sentence-transformers) (2.4.7) Requirement already satisfied: zipp&gt;=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata-&gt;transformers&lt;5.0.0,&gt;=4.6.0-&gt;sentence-transformers) (3.6.0) Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk-&gt;sentence-transformers) (1.15.0) Requirement already satisfied: idna&lt;3,&gt;=2.5 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers&lt;5.0.0,&gt;=4.6.0-&gt;sentence-transformers) (2.10) Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,&lt;1.26,&gt;=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers&lt;5.0.0,&gt;=4.6.0-&gt;sentence-transformers) (1.24.3) Requirement already satisfied: certifi&gt;=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers&lt;5.0.0,&gt;=4.6.0-&gt;sentence-transformers) (2021.10.8) Requirement already satisfied: chardet&lt;4,&gt;=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests-&gt;transformers&lt;5.0.0,&gt;=4.6.0-&gt;sentence-transformers) (3.0.4) Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers&lt;5.0.0,&gt;=4.6.0-&gt;sentence-transformers) (7.1.2) Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses-&gt;transformers&lt;5.0.0,&gt;=4.6.0-&gt;sentence-transformers) (1.1.0) Requirement already satisfied: threadpoolctl&gt;=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn-&gt;sentence-transformers) (3.0.0) Requirement already satisfied: pillow!=8.3.0,&gt;=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision-&gt;sentence-transformers) (7.1.2) Building wheels for collected packages: sentence-transformers Building wheel for sentence-transformers (setup.py) ... done Created wheel for sentence-transformers: filename=sentence_transformers-2.1.0-py3-none-any.whl size=121000 sha256=f7a209998618704c344dad092c6593fe0ad32927fe12546dab31670c8027aaaa Stored in directory: /root/.cache/pip/wheels/90/f0/bb/ed1add84da70092ea526466eadc2bfb197c4bcb8d4fa5f7bad Successfully built sentence-transformers Installing collected packages: pyyaml, tokenizers, sacremoses, huggingface-hub, transformers, sentencepiece, sentence-transformers Attempting uninstall: pyyaml Found existing installation: PyYAML 3.13 Uninstalling PyYAML-3.13: Successfully uninstalled PyYAML-3.13 Successfully installed huggingface-hub-0.1.2 pyyaml-6.0 sacremoses-0.0.46 sentence-transformers-2.1.0 sentencepiece-0.1.96 tokenizers-0.10.3 transformers-4.12.3 . Load CLIP Model . Next we load the CLIP model using SentenceTransformer. The model is downloaded automatically. . import sentence_transformers from sentence_transformers import SentenceTransformer, util from PIL import Image import glob import torch import pickle import zipfile from IPython.display import display from IPython.display import Image as IPImage import os from tqdm.autonotebook import tqdm torch.set_num_threads(4) #First, we load the respective CLIP model model = SentenceTransformer(&#39;clip-ViT-B-32&#39;) . ftfy or spacy is not installed using BERT BasicTokenizer instead of ftfy. . img_folder = &#39;photos/&#39; if not os.path.exists(img_folder) or len(os.listdir(img_folder)) == 0: os.makedirs(img_folder, exist_ok=True) photo_filename = &#39;unsplash-25k-photos.zip&#39; if not os.path.exists(photo_filename): #Download dataset if does not exist util.http_get(&#39;http://sbert.net/datasets/&#39;+photo_filename, photo_filename) #Extract all images with zipfile.ZipFile(photo_filename, &#39;r&#39;) as zf: for member in tqdm(zf.infolist(), desc=&#39;Extracting&#39;): zf.extract(member, img_folder) . # To speed things up, we destribute pre-computed embeddings # Otherwise you can also encode the images yourself. # To encode an image, you can use the following code: # from PIL import Image # img_emb = model.encode(Image.open(filepath)) use_precomputed_embeddings = True if use_precomputed_embeddings: emb_filename = &#39;unsplash-25k-photos-embeddings.pkl&#39; if not os.path.exists(emb_filename): #Download dataset if does not exist util.http_get(&#39;http://sbert.net/datasets/&#39;+emb_filename, emb_filename) with open(emb_filename, &#39;rb&#39;) as fIn: img_names, img_emb = pickle.load(fIn) print(&quot;Images:&quot;, len(img_names)) else: img_names = list(glob.glob(&#39;unsplash/photos/*.jpg&#39;)) print(&quot;Images:&quot;, len(img_names)) img_emb = model.encode([Image.open(filepath) for filepath in img_names], batch_size=128, convert_to_tensor=True, show_progress_bar=True) . Images: 24996 . def search(query, k=3): # First, we encode the query (which can either be an image or a text string) query_emb = model.encode([query], convert_to_tensor=True, show_progress_bar=False) # Then, we use the util.semantic_search function, which computes the cosine-similarity # between the query embedding and all image embeddings. # It then returns the top_k highest ranked images, which we output hits = util.semantic_search(query_emb, img_emb, top_k=k)[0] print(&quot;Query:&quot;) display(query) for hit in hits: print(img_names[hit[&#39;corpus_id&#39;]]) display(IPImage(os.path.join(img_folder, img_names[hit[&#39;corpus_id&#39;]]), width=200)) . search(&quot;Two cats playing on the street&quot;) . Query: . &#39;Two cats playing on the street&#39; . 4mA9_5vbZ_s.jpg . w6tMRf7kGLA.jpg . n4pNuXxyIr4.jpg . search(&quot;A sunset on the montain&quot;) . Query: . &#39;A sunset on the montain&#39; . Zf4jpcGEinM.jpg . G5JDRSKi3uY.jpg . ig9yVlj5YYg.jpg . search(&quot;Oslo&quot;) . Query: . &#39;Oslo&#39; . uHsQou9tWTQ.jpg . 0ABCZ9bTsw4.jpg . d5_hjWQ4NwA.jpg . search(&quot;A dog in a park&quot;) . Query: . &#39;A dog in a park&#39; . IVyZrLp41D0.jpg . 0O9A0F_d1qA.jpg . KVeogBZzl4M.jpg . search(&quot;A beach with palm trees&quot;) . Query: . &#39;A beach with palm trees&#39; . 7rrgPPljqYU.jpg . kmihWgpbDEg.jpg . ZyfOq52b0cs.jpg . Image-to-Image Search . You can use the method also for image-to-image search. . To achieve this, you pass Image.open(&#39;path/to/image.jpg&#39;) to the search method. . It will then return similar images . img_test_folder = &#39;test/&#39; search(Image.open(os.path.join(img_test_folder, &#39;branca.jpg&#39;)), k=5) . Query: . mgOXIPnyIxg.jpg . L8PkoOGLMSk.jpg . 0T3R-QVgXUg.jpg . JqwRXYpSMDY.jpg . gcE6vwqOBjY.jpg .",
            "url": "https://marcelcastrobr.github.io/notebooks/2022/01/03/Image_Search.html",
            "relUrl": "/2022/01/03/Image_Search.html",
            "date": " • Jan 3, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Machine Learning Model Monitoring",
            "content": "Machine Learning Model Monitoring . . Why monitoring matters: . Machine learning model monitoring is important as it allows to check for changes on the model performance. It is a cyclical and interactive process and need also to consider the monitoring of the infrastructure such as database and application. . Model monitoring should account for: . Data skews . | Model staleness . | Negative feedback loops . | . Functional and non-functional monitoring points are: . Functional: Predictive peformance | Changes in serving data | Metrics used during training | Characteristics of features | . | Non-functional System performance | System status | System reliability | . | . Concepts . Data Skew: . Data skews occurs when the model training data is not representative of the live data. There are several reasons for data skew, such as: . Training data was designed wrong such as the distribution of the features in the training is different from the distribution of the features in real life data. | Feature not available in production | . Model Staleness . Model staleness can occur based on: . Shifts in the environment as historic data used during model training may change as time progress (e.g. financial models using time of recession might not be effective for predicting default when economy is healthy). | Consumer behaviour change such as trends in politics, fashion, etc. | Adversarial scenarios where bad actors (e.g. criminals) seek to weaken the model. | . Negative feedback loops . Negative feedback loop arises when you train data collected in production that can lead to bias. . Model Decay . Production ML models often operation in dynamic environments (e.g. recommendation system of clothes need to change over time as the clothes style change over time. . If the. Model is static, it will move further away from the truth, issue known as Model drift. Model drift can be split in: . Data drift: statistical properties of the input features changes. (e.g. distribution of age feature in a population over time). Real examples here and here. | Concept drift: occurs when the relationship between the features and labels changes. Examples are prediction drift and label drift. A real example here. | . What and How to Monitor in ML models: . WHAT should we monitor in an ML model in production: . Model input distribution Errors: input values fall within an allowed set/range? | Changes: does the distribution align with what was seen during training? | . | Model prediction distribution Statistical significance: e.g. if variables are normally distributes, we might expect the mean values to be within the standard euro of the mean interval. | . | Model versions | Input/prediction correlation | . HOW should we monitor it: . Tracing your ML model through logging. Observability of ML model while logging distributed tracings might be challenging. However, tools like Dapper, Zipkin and Jaeger could help to do the job. | . | Detecting drift: Check for statistical properties of the logged data, model predictions and possibly ground truth over time. Examples of tools that can be used are TensorFlow data validation (TFDV), scikit-multiflow library, or Google Vertex prediction. | What if Drift is detected: Determine the portion of your training dataset that is still correct. | Keep good data and discard the bad. | Create an entirely new training dataset from the new data. | . | When to retrain my model: On demand -&gt; manual retrain the model | On schedule -&gt; when new labelled data is available at a daily/weekely/yearly basis | Availability of new training data -&gt; new data is available on an ad-hoc basis. | . | . | . References: . [1] Deploying Machine Learning in Production, Deeplearning . [2] MLOps: What It Is, Why It Matters, and How to Implement It . [3] Awesome MLOps . [4] Retraining Model During Deployment: Continuous Training and Continuous Testing .",
            "url": "https://marcelcastrobr.github.io/notebooks/ml,/data/drift,/model/mlops/2021/12/13/ModelMonitoring.html",
            "relUrl": "/ml,/data/drift,/model/mlops/2021/12/13/ModelMonitoring.html",
            "date": " • Dec 13, 2021"
        }
        
    
  
    
        ,"post4": {
            "title": "Github Actions and Snowflake Integration",
            "content": "GitHub Actions and Snowflake Integration . The objective of this article is to explore CICD (Continuous Integration / Continuous Deployment) for our data modelling using Github actions and Snowflake data platform. . The benefit of applying CICD concept on your data model is to keep traceability of your model and easy deployment to your data platform, which in this case is Snowflake. . GitHub is used as a repository for the data model. The data model is a collection of sql queries used to generated tables and views. . Authentication: . In this setup, two authentication methods towards Snowflake were tested: using of AzureAD Single Sign On (a.k.a. SSO) and Key-pair authentication. But for the remaining of this article, the key-pair authentication towards Snowflake was used. . Using Key-pair Authentication . In order to use key-pair authentication, we need the following steps: . 1) Create a key-pair key (private and public key) to be used to Authenticate towards snowflake while running the queries. 2) Load the private key unders the secrets of your repository on gitHub used to capture your data model. 3) Load your public key to your snowflake account. . Below are the command needs for each step. . Step1 : Creating encrypted key . Using a a linux command line (CLI) you can generate your private (rsa_key.p8) an public key (rsa_key.pub) executing the commands below (ref. Snowflake docs). . I decided to use encrypted private key, which means I created an additional passphrase needed while using the key-pair. . # Create a private encrypted key $ openssl genrsa 2048 | openssl pkcs8 -topk8 -inform PEM -out rsa_key.p8 # Create a public key $ openssl rsa -in rsa_key.p8 -pubout -out rsa_key.pub . | https://docs.snowflake.com/en/user-guide/key-pair-auth.html . Export your private key (rsa_key.p8) and password/passphrase to GitHub Actions under Settings-&gt; Secrets (secrets SF_PRIVATE_KEY and and SF_PRIVATE_PASSPHRASE in picture below): . . | Write your public key (rsa_key.pub) to your username in snowflake using the command below in Snowflake console: . #Adding public key to snowflake user. You can add a second key also using RSA_PUBLIC_KEY_2 ALTER USER &quot;&lt;your user&gt;&quot; SET RSA_PUBLIC_KEY=&quot;&lt; your public key&gt;&quot;; #You can check if key was update by issuing the following command: DESC USER &quot;&lt;your user&gt;&quot;; . | . Beside the key-pair, you also need to pass the authentication parameters needed by snowsql, which are listed below. We will describe snowsql later in this section. . SF_ACCOUNT: | SF_DATABASE: | SF_ROLE: | SF_USERNAME: | SF_WAREHOUSE: | . Interacting with Snowflake . There are several ways to interact with snowflake. In this notebook, I have tested: . Snowflake web console | Snowsql | . Using Snowsql . Snowsql is the command line for connecting to Snowflake to execute SQL queries and perform all DDL and DML operations, including loading data into and unloading data out of database tables (ref. SnowSQL (CLI Client)). . Snowsql allows us to use both Single Sign On (SSO) and key-value pair. Below are the commands needed. . Snowsql with SSO . $ snowsql -a &lt;snowflake_account&gt; -u &quot;&lt;snowflake_username&gt;&quot; --authenticator externalbrowser . Snowsql with key-pair . #Getting snowsql cli $ snowsql -a &lt;snowflake_account&gt; -u &quot;&lt;snowflake_username&gt;&quot; --private-key-path ~/.ssh/snowflake-key-private.p8 #Running specific sql file named myfile.sql $ snowsql -a &lt;snowflake_account&gt; -u &quot;&lt;snowflake_username&gt;&quot; --private-key-path ~/.ssh/snowflake-key-private.p8 -f scripts/myfile.sql . See the Snowflake documentation https://docs.snowflake.net/manuals/user-guide/snowsql.html for more information on how to interact with snowsql. . Connector Python Library . In addition to snowsql, you can also interact with Snowflake using the snowflake connector python library. . There are two ways of executing queries with the library - execute for synchronous execution and execute_async for asynchronous ones. . The synchronous way is simple and good for a batch of quick/dependent queries. But, if you have independent queries, some of which may take a long time to run, there is no reason to block each other and use the synchronous way (ref. Article). . You can use Snowflake-query GitHub action to run your queries. . GitHub Actions . In this section we show how to configure GitHub actions through a simple workflow. Thereafter we run the workflow manually and the its execution. . Configuring the pipeline workflow . The code below is our data pipeline workflow named snowflake-devops-demo.yml. The workflow is a yaml file located inside the path snowflake-datapipeline/.github/workflows/.Please note that github actions can run several workflow, we only need to create different yaml files within the workflow folder. . # **snowflake-devops-demo.yml.** name: snowflake-devops-demo # Environment variable created under github secrets. env: SF_ACCOUNT: $ SF_USERNAME: $ SF_ROLE: $ SF_WAREHOUSE: $ SF_DATABASE: $ SNOWFLAKE_PRIVATE_KEY: $ SNOWFLAKE_PRIVATE_KEY_PASSPHRASE: $ SNOWSQL_PRIVATE_KEY_PASSPHRASE: $ # Controls when the action will run. In this case for branch main under changes under the path migrations. on: push: branches: - main paths: - &#39;migrations/**&#39; # Allows you to run this workflow manually from the Actions tab workflow_dispatch: jobs: deploy-snowflake-changes-job: runs-on: ubuntu-latest steps: # Checks-out your repository under $GITHUB_WORKSPACE, so your job can access it - name: Checkout repository uses: actions/checkout@v2 # Copying our private key under ~/.ssh/ - name: Install SSH Key uses: shimataro/ssh-key-action@v2 with: key: $ name: id_rsa-snowflake known_hosts: &#39;just-a-placeholder-so-we-dont-get-errors&#39; - name: Use Python 3.8.x uses: actions/setup-python@v2.2.1 with: python-version: 3.8.x - name: Download SnowSQL run: curl -O https://sfc-repo.snowflakecomputing.com/snowsql/bootstrap/1.2/linux_x86_64/snowsql-1.2.19-linux_x86_64.bash - name: Install SnowSQL run: SNOWSQL_DEST=~/snowflake SNOWSQL_LOGIN_SHELL=~/.profile bash snowsql-1.2.19-linux_x86_64.bash - name: Test installation run: ~/snowflake/snowsql -v - name: Execute sql files against Snowflake run: | echo &quot;Creating SNOWFLAKE_PRIVATE_KEY_PATH variable&quot; export SNOWFLAKE_PRIVATE_KEY_PATH=~/.ssh/id_rsa-snowflake echo &quot;Step 3: Executing snowsql&quot; # Give permission to execute bash script chmod u+x $GITHUB_WORKSPACE/simple_integration.sh $GITHUB_WORKSPACE/simple_integration.sh . The code below is the content of the simple_integration.sh script, which reads a workflow.conf file and execute each line in snowflake using the snowsql commands. . # simple_integration.sh script that reads workflow.conf file and execute it. #!/bin/bash # author: Marcel Castro set -e print_log () { printf &quot;[`date +&#39;%d/%m/%Y %H:%M:%S&#39;`] [$1] $2 n&quot; } export -f print_log run_workflow () { print_log &quot;INFO&quot; &quot;Running workflow&quot; workflow_config=$(echo sed -e &#39;s/#.*$//&#39; -e &#39;/^$/d&#39; workflow.conf) while IFS= read -r sql_file_name; do print_log &quot;INFO&quot; &quot;Running: $sql_file_name&quot; print_log &quot;INFO&quot; &quot;Running ~/snowflake/snowsql -a $SF_ACCOUNT -u $SF_USERNAME -r $SF_ROLE -w $SF_WAREHOUSE -d $SF_DATABASE --private-key-path $SNOWFLAKE_PRIVATE_KEY_PATH -f ${GITHUB_WORKSPACE}/${sql_file_name}&quot; ~/snowflake/snowsql -a $SF_ACCOUNT -u $SF_USERNAME -r $SF_ROLE -w $SF_WAREHOUSE -d $SF_DATABASE --private-key-path $SNOWFLAKE_PRIVATE_KEY_PATH -f ${GITHUB_WORKSPACE}/${sql_file_name} done &lt; &lt;($workflow_config); } ## running workflow run_workflow . An example of workflow.conf file can be: . # Create views views/my_first_view.sql views/my_second_view.sql #Create procedures procedures/my_first_procedure.sql # Remember to have an empty line by the end at the end of the file # so that every line is processed . Running the pipeline workflow . You can either run your workflow manually as highlighted in the picture below or triggered by a commit to your repository. . . After some minutes you see that the workflow run successfully. The picture below shows the results of each steps within the workflow. . . Summary . In this article we explored a simple data workflow using GitHub actions and Snowflake. Github is used a code repository to version our data model. . We introduce the use of GitHub actions as a CICD (Continuous Integration / Continuous Deployment) data pipeline which deploys our simple data model on snowflake through the use of snowsql commands. . The deployment is done in a secure way through the use of key pairs stored in the GitHub actions secrets. . In the future work, I would like to explore the use of dbt (data build tool) to perform data transformation. The idea is to make use of the dbt test and documentation capabilities. .",
            "url": "https://marcelcastrobr.github.io/notebooks/github,/snowflake,/cicd/2021/10/31/githubactions-snowflake.html",
            "relUrl": "/github,/snowflake,/cicd/2021/10/31/githubactions-snowflake.html",
            "date": " • Oct 31, 2021"
        }
        
    
  
    
        ,"post5": {
            "title": "Dimensionality Reduction - Non-negative Matrix Factorization - NMF",
            "content": "About . Notebook inspired by the example of Dimensionality Reduction done in the course Machine Learning Modeling Pipelines in Production, by DeepLearning.AI. . Notebook Reference: https://colab.research.google.com/drive/1Cf0JMZ9RQdpmwsJ0PxSxm6IKYtrWr02j#scrollTo=MPsBWkL07Srw. . [25-Oct-2021] This notebook was extended to capture background knowledge from NLP from the CS224U 2019 - Natural Language Understanding course from Stanford CS224U-2019 - Natural Language Understanding - Stanford Univerity. . Background: . Vector Comparison . Common vector comparison measures are: . Euclidean distance between vectors u and v: $euclidean(u,v) = sqrt{ sum_{i=1}^{n} left | u_{i}- v_{i}] right |^2}$ | Cosine distance | Matching coefficient | Jaccard distance | Dice distance | Overlap | KL divergence | . Based on Christopher Potts: . Euclidean and Jaccard and Dice with raw count vectors will tend to favor raw frequency over distributional patters | Euclidean with L2-normed vectors is equivalent to cosine with respect to ranking. | Jaccard and Dice are equivalent w.r.t. ranking. | . Basic reweighting . Reweighting of vectors in natural language can amplify the importance and trustworthy of words in a text. The idea is to reveal what is important. Methods for reweighting are: . L2 norming | Probability distribution | Observed/expected | Pointwise Mutual Information (PMI) | Positive PMI. | TF-IDF: Term Frequency - Inverse Document Frequency | . Pointwise Mutual Information (PMI) is observed/expected in the log-space (with log(0) = 0. Based on Christopher Potts: . Many weighting schemes end up favoring rare events that may not be trustworthy. | PMI and its variant will amplify the values of counts that are tiny relative to their row and columns. | TF-IDF severly punishes words that appear in many documents - it behaves oddly for dense matrices, which can include word x word matrices. | . import numpy as np import pandas as pd import seaborn as sns import matplotlib.pyplot as plt import math import sys . from sklearn.feature_extraction.text import TfidfVectorizer from sklearn.decomposition import NMF from sklearn.datasets import fetch_20newsgroups . # Source github link is here: https://github.com/cgpotts/cs224u # Instead of using the word-to-word matrixes from CS224u, we used the dataset from 20newsgroups from sklearn and # generate the co-occurance matrix for the 100 first rows of this dataset. import os import vsm import utils . # Test using 20newsgroups dataset from https://scikit-learn.org/stable/datasets/real_world.html#the-20-newsgroups-text-dataset # Download data data = fetch_20newsgroups(remove=(&#39;headers&#39;, &#39;footers&#39;, &#39;quotes&#39;)) # Get the actual text data from the sklearn Bunch data = data.get(&quot;data&quot;) . print(data[:1]) . [&#39;I was wondering if anyone out there could enlighten me on this car I saw nthe other day. It was a 2-door sports car, looked to be from the late 60s/ nearly 70s. It was called a Bricklin. The doors were really small. In addition, nthe front bumper was separate from the rest of the body. This is nall I know. If anyone can tellme a model name, engine specs, years nof production, where this car is made, history, or whatever info you nhave on this funky looking car, please e-mail.&#39;] . from sklearn.feature_extraction.text import CountVectorizer vectorizer = CountVectorizer(ngram_range=(1,1)) # default unigram model X = vectorizer.fit_transform(data[:100]) Xc = (X.T * X) # matrix manipulation Xc.setdiag(0) # set the diagonals to be zeroes as it&#39;s pointless to be 1 #Convert into matrix names = vectorizer.get_feature_names() # This are the entity names (i.e. keywords) #print(names) df = pd.DataFrame(data = Xc.toarray(), columns = names, index = names) #df.to_csv(&#39;20newsgroups.csv&#39;, sep = &#39;,&#39;) df.head() . 00 000 0005895485 01 02 02194 0320 04 05 06 ... ysc yscvax zangezour zangibasar zealand zeik zilkade zone zoologists zoom . 00 0 | 0 | 0 | 6 | 4 | 0 | 0 | 4 | 6 | 5 | ... | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 000 0 | 0 | 0 | 0 | 0 | 0 | 5 | 0 | 0 | 0 | ... | 0 | 0 | 5 | 5 | 0 | 0 | 5 | 0 | 0 | 0 | . 0005895485 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | ... | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 01 6 | 0 | 0 | 0 | 24 | 0 | 0 | 24 | 36 | 30 | ... | 6 | 6 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 02 4 | 0 | 0 | 24 | 0 | 0 | 0 | 16 | 24 | 20 | ... | 4 | 4 | 0 | 0 | 2 | 0 | 0 | 1 | 0 | 0 | . 5 rows × 4906 columns . Lets compare different distance metrics (i.e. Euclidean, Pointwise Mutual Information and TF-IDF . vsm.neighbors(&#39;car&#39;, df, distfunc=vsm.euclidean).head() . car 0.000000 insurance 442.483898 25 443.953826 rate 530.062260 same 540.973197 dtype: float64 . df_pmi = vsm.pmi(df) vsm.neighbors(&#39;car&#39;, df_pmi).head() . car 0.000000 sports 0.134340 turbo 0.153308 toyota 0.162001 accidents 0.173207 dtype: float64 . df_tfidf = vsm.tfidf(df) vsm.neighbors(&#39;car&#39;, df_tfidf).head() . car 0.000000 sports 0.214020 turbo 0.362690 classification 0.369355 corrado 0.369355 dtype: float64 . vsm.tsne_viz(df_pmi, output_filename=&#39;pmi-test&#39;) . Dimensionality Reduction . The &quot;curse of dimensionaliy&quot; is a known concept on machine learning (ML). In summary it relates to the fact that too many features can be a problem for ML algorithms. . Examples of dimentionality reductions are: . Latent Semantic Analysis (LSA) | Principa Component Analysis (PCA) | Latent Dirichlet Allocation (LDA) | Non-Negative Matrix Factorization (NMF) | Independent Component Analysis (ICA) | Singular value decomposition (SVD) | . Use sklearn.decomposition and sklearn.manifold for more information. . NMF - non-negative matrix factorization. . In this section of the notebook I am going to concentrate on NMF - non-negative matrix factorization. . NMF is a dimensionality reduction technique in unsupervised learning, but in contrast with PCA, the NMF model is interpretable. NMF requires the sample features to be non-negative, thus NMF can tend to lose more information when truncating. . NMF in text minining consider the bag-of-words matrix representation where each row corresponds to a word, and each column to a document. . NMF will produce two matrices W and H, thus we can write V = W x H. According to COLYER, the columns W can be interpreted as the terms to topic information (i.e. topic/bags of words). H represents the importance of the given topic to a a given document. . We can be write NMF as: term-document matrix (V) = terms/topics (W) + topics/docs (H) . In practise, the inputs is: . count vectorizer or TF-IDF vectorizer | . the parameters to tune are: . Number of topics | text preprocessing (e.g. stop words, min/max doc frequency, parts of speech) | . the output: . W matrix representing relation between terms and topics | H matrix representing how to use the topics to reconstruct original documents (i.e. documents to topics relation) | . . References: . [1] NML by IBM in coursera . [2] COLYER . [3] CS224U-2019 - Natural Language Understanding - Stanford Univerity . Helping functions . def plot_words_for_topics(n_comp, nmf, feature_names): fig, axes = plt.subplots(((n_comp-1)//5)+1, 5, figsize=(25, 15)) axes = axes.flatten() for num_topic, topic in enumerate(nmf.components_, start=1): # Plot only the top 20 words # Get the top 20 indexes top_indexes = np.flip(topic.argsort()[-20:]) # Get the corresponding feature name top_features = [feature_names[i] for i in top_indexes] # Get the importance of each word importance = topic[top_indexes] # Plot a barplot ax = axes[num_topic-1] ax.barh(top_features, importance, color=&quot;green&quot;) ax.set_title(f&quot;Topic {num_topic}&quot;, {&quot;fontsize&quot;: 20}) ax.invert_yaxis() ax.tick_params(labelsize=15) plt.tight_layout() plt.show() # Run the function #plot_words_for_topics(n_comp, nmf, feature_names) . 20newsgroups dataset . # Download data data = fetch_20newsgroups(remove=(&#39;headers&#39;, &#39;footers&#39;, &#39;quotes&#39;)) # Get the actual text data from the sklearn Bunch data = data.get(&quot;data&quot;) . print(f&quot;Data has {len(data)} elements. n&quot;) print(f&quot;First 2 elements: n&quot;) for n, d in enumerate(data[:1], start=1): print(&quot;======&quot;*10) print(f&quot;Element number {n}: n n{d} n&quot;) . Data has 11314 elements. First 2 elements: ============================================================ Element number 1: I was wondering if anyone out there could enlighten me on this car I saw the other day. It was a 2-door sports car, looked to be from the late 60s/ early 70s. It was called a Bricklin. The doors were really small. In addition, the front bumper was separate from the rest of the body. This is all I know. If anyone can tellme a model name, engine specs, years of production, where this car is made, history, or whatever info you have on this funky looking car, please e-mail. . def get_vector(data): # Convert a collection of raw documents to a matrix of TF-IDF features. #vectorizer = TfidfVectorizer(max_features=500, stop_words=&#39;english&#39;) # max_df: when building the vocabulary ignore terms that have a document frequency strictly higher than the given threshold # min_df: when building the vocabulary ignore terms that have a document frequency strictly lower than the given threshold vectorizer = TfidfVectorizer(max_df=0.95, min_df=2, max_features=500, stop_words=&#39;english&#39;) # Vectorize original data vect_data = vectorizer.fit_transform(data) # Print dimensionality print(f&quot;Data has shape {vect_data.shape} after vectorization.&quot;) print(f&quot;Each data point has shape {vect_data[0].shape} after vectorization.&quot;) return vectorizer, vect_data . def try_NMF(n_comp, data): &#39;&#39;&#39; input: n_comp = number of components vect_data = data matrix to be decomposed output: plot words for topics &#39;&#39;&#39; vectorizer, vect_data = get_vector(data) nmf = NMF(n_components=n_comp, random_state=42) # Lets learn a NMF model for the vect_data nmf.fit(vect_data) feature_names = vectorizer.get_feature_names() plot_words_for_topics(n_comp, nmf, feature_names) . try_NMF(5, data) . Data has shape (11314, 500) after vectorization. Each data point has shape (1, 500) after vectorization. . /Users/castrma/miniconda3/envs/tensorflow_py37/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:315: FutureWarning: The &#39;init&#39; value, when &#39;init=None&#39; and n_components is less than n_samples and n_features, will be changed from &#39;nndsvd&#39; to &#39;nndsvda&#39; in 1.1 (renaming of 0.26). &#34;&#39;nndsvda&#39; in 1.1 (renaming of 0.26).&#34;), FutureWarning) . Non-conformance dataset (internal dataset) . dict_csv = pd.read_csv(&#39;/Users/castrma/Projects/DT/svt-data-processing/others/csv/notification_long_text.csv&#39;, header=None, index_col=0, squeeze=True).to_dict() #print(dict_csv) . internal_data = dict_csv.values() #Remove NaN from list internal_data = [x for x in internal_data if pd.isnull(x) == False] . print(f&quot;Data has {len(internal_data)} elements. n&quot;) print(f&quot;First element: n&quot;) #for n, d in enumerate(internal_data[:1], start=1): # print(&quot;======&quot;*10) # print(f&quot;Element number {n}: n n{d} n&quot;) . Data has 386102 elements. First element: . try_NMF(5, internal_data) . Data has shape (386102, 500) after vectorization. Each data point has shape (1, 500) after vectorization. . #print(next(iter(data))) #print(f&quot;Representation based on topics: n{vect_data_internal[0]}&quot;) . def NMFv2(n_comp, data): &#39;&#39;&#39; input: n_comp = number of components vect_data = data matrix to be decomposed output: plot words for topics &#39;&#39;&#39; vectorizer, vect_data = get_vector(data) #beta_loss: beta divergence to be minimized #solver: numerical solver to use. mu = Multiplicative Update solver #max_iter: maximum number of iterations before timing out. #alpha: constant that multiplies the regularization terms #l1_ratio: the regularization mixing parameter nmf = NMF(n_components=n_comp, random_state=42, beta_loss=&#39;kullback-leibler&#39;, solver=&#39;mu&#39;, max_iter=1000, alpha=.1, l1_ratio=.5) # Lets learn a NMF model for the vect_data nmf.fit(vect_data) feature_names = vectorizer.get_feature_names() plot_words_for_topics(n_comp, nmf, feature_names) . NMFv2(5, internal_data) . Data has shape (386102, 500) after vectorization. Each data point has shape (1, 500) after vectorization. . /Users/castrma/miniconda3/envs/tensorflow_py37/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:315: FutureWarning: The &#39;init&#39; value, when &#39;init=None&#39; and n_components is less than n_samples and n_features, will be changed from &#39;nndsvd&#39; to &#39;nndsvda&#39; in 1.1 (renaming of 0.26). &#34;&#39;nndsvda&#39; in 1.1 (renaming of 0.26).&#34;), FutureWarning) .",
            "url": "https://marcelcastrobr.github.io/notebooks/jupyter/nmf/dimensionality/tf-idf/pmi/2021/10/25/NMF.html",
            "relUrl": "/jupyter/nmf/dimensionality/tf-idf/pmi/2021/10/25/NMF.html",
            "date": " • Oct 25, 2021"
        }
        
    
  
    
        ,"post6": {
            "title": "Principal Component Analysis - PCA",
            "content": "from IPython.display import Image . Background . Data compression is an important topic of machine learning, as it allows us to analyse and interpret big amount of data. In ML, feature extraction techniques allows us to reduce the number of features in a dataset. Different from feature selection whchig maintain the original features, feature extraction transform or project the data onto new feature spaces. . Feature extraction improves the predictive performance of a given model by reducing the curse of dimensionality. PCA finds correlation between the features by finding the directions of maximum variance in high-dimentional data and it projects the data onto a new subspace with equal or fewer dimensions. The ortogonal axes (principal components) of the new subspace should be interpreted as the directions of maximum variance given the constraint constraint that the new feature axes are ortogonal to each other [1]. . Image(filename=&#39;images/GaussianScatterPCA.png&#39;) . Note that PCA directions are high sensitive to data scaling, thus the need to standardize the features prior to PCA. . As described in [1], The main steps behing PCA are: . 1) Standardize the d-dimentional dataset 2) Construct the covariance matrix 3) Decompose the covariance matrix into its eigenvectors and eigenvalues 4) Sort the eigenvalues by decreasing order to rank the corresponding eigenvectors 5) Select k eigeinvectors, which correspond to the k largest eigenvalues, where k is the dimensionality of the new feature space ($(k &lt;= d)) 6) Construct a projection matrix, W, from the top k eigenvectors. 7) Transform the d-dimensional input dataset, X, using the projection matrix W, to obtain the new k-dimensional feature space. . [1] Python Machine Learning - by Sebastian Raschka [2] Python Machine Learning - Code Example Chapter 5 - https://github.com/rasbt/python-machine-learning-book-3rd-edition/blob/master/ch05/ch05.ipynb . import pandas as pd df_wine = pd.read_csv(&#39;https://archive.ics.uci.edu/ml/&#39; &#39;machine-learning-databases/wine/wine.data&#39;, header=None) . df_wine.columns = [&#39;Class label&#39;, &#39;Alcohol&#39;, &#39;Malic acid&#39;, &#39;Ash&#39;, &#39;Alcalinity of ash&#39;, &#39;Magnesium&#39;, &#39;Total phenols&#39;, &#39;Flavanoids&#39;, &#39;Nonflavanoid phenols&#39;, &#39;Proanthocyanins&#39;, &#39;Color intensity&#39;, &#39;Hue&#39;, &#39;OD280/OD315 of diluted wines&#39;, &#39;Proline&#39;] df_wine.head() . Class label Alcohol Malic acid Ash Alcalinity of ash Magnesium Total phenols Flavanoids Nonflavanoid phenols Proanthocyanins Color intensity Hue OD280/OD315 of diluted wines Proline . 0 1 | 14.23 | 1.71 | 2.43 | 15.6 | 127 | 2.80 | 3.06 | 0.28 | 2.29 | 5.64 | 1.04 | 3.92 | 1065 | . 1 1 | 13.20 | 1.78 | 2.14 | 11.2 | 100 | 2.65 | 2.76 | 0.26 | 1.28 | 4.38 | 1.05 | 3.40 | 1050 | . 2 1 | 13.16 | 2.36 | 2.67 | 18.6 | 101 | 2.80 | 3.24 | 0.30 | 2.81 | 5.68 | 1.03 | 3.17 | 1185 | . 3 1 | 14.37 | 1.95 | 2.50 | 16.8 | 113 | 3.85 | 3.49 | 0.24 | 2.18 | 7.80 | 0.86 | 3.45 | 1480 | . 4 1 | 13.24 | 2.59 | 2.87 | 21.0 | 118 | 2.80 | 2.69 | 0.39 | 1.82 | 4.32 | 1.04 | 2.93 | 735 | . from sklearn.model_selection import train_test_split # First column - class label is the target X, y = df_wine.iloc[:, 1:].values, df_wine.iloc[:, 0].values X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, stratify=y, random_state=0) . 1) Standardize the d-dimentional dataset . from sklearn.preprocessing import StandardScaler sc = StandardScaler() X_train_std = sc.fit_transform(X_train) X_test_std = sc.transform(X_test) . 2) Construct the covariance matrix . 3) Decompose the covariance matrix into its eigenvectors and eigenvalues . import numpy as np cov_mat = np.cov(X_train_std.T) eigen_vals, eigen_vecs = np.linalg.eig(cov_mat) print(&#39; nEigenvalues n%s&#39; % eigen_vals) . Eigenvalues [4.84274532 2.41602459 1.54845825 0.96120438 0.84166161 0.6620634 0.51828472 0.34650377 0.3131368 0.10754642 0.21357215 0.15362835 0.1808613 ] . 4) Sort the eigenvalues by decreasing order to rank the corresponding eigenvectors . tot = sum(eigen_vals) var_exp = [(i / tot) for i in sorted(eigen_vals, reverse=True)] cum_var_exp = np.cumsum(var_exp) . import matplotlib.pyplot as plt plt.bar(range(1, 14), var_exp, alpha=0.5, align=&#39;center&#39;, label=&#39;Individual explained variance&#39;) plt.step(range(1, 14), cum_var_exp, where=&#39;mid&#39;, label=&#39;Cumulative explained variance&#39;) plt.ylabel(&#39;Explained variance ratio&#39;) plt.xlabel(&#39;Principal component index&#39;) plt.legend(loc=&#39;best&#39;) plt.tight_layout() # plt.savefig(&#39;images/05_02.png&#39;, dpi=300) plt.show() . 5) Select k eigeinvectors, which correspond to the k largest eigenvalues, where k is the dimensionality of the new feature space ($(k &lt;= d)) . eigen_pairs = [(np.abs(eigen_vals[i]), eigen_vecs[:, i]) for i in range(len(eigen_vals))] # Sort the (eigenvalue, eigenvector) tuples from high to low eigen_pairs.sort(key=lambda k: k[0], reverse=True) . 6) Construct a projection matrix, W, from the top k eigenvectors. . w = np.hstack((eigen_pairs[0][1][:, np.newaxis], eigen_pairs[1][1][:, np.newaxis])) print(&#39;Matrix W: n&#39;, w) . Matrix W: [[-0.13724218 0.50303478] [ 0.24724326 0.16487119] [-0.02545159 0.24456476] [ 0.20694508 -0.11352904] [-0.15436582 0.28974518] [-0.39376952 0.05080104] [-0.41735106 -0.02287338] [ 0.30572896 0.09048885] [-0.30668347 0.00835233] [ 0.07554066 0.54977581] [-0.32613263 -0.20716433] [-0.36861022 -0.24902536] [-0.29669651 0.38022942]] . 7) Transform the d-dimensional input dataset, X, using the projection matrix W, to obtain the new k-dimensional feature space. . X_train_std[0].dot(w) . array([2.38299011, 0.45458499]) . X_train_pca = X_train_std.dot(w) colors = [&#39;r&#39;, &#39;b&#39;, &#39;g&#39;] markers = [&#39;s&#39;, &#39;x&#39;, &#39;o&#39;] for l, c, m in zip(np.unique(y_train), colors, markers): plt.scatter(X_train_pca[y_train == l, 0], X_train_pca[y_train == l, 1], c=c, label=l, marker=m) plt.xlabel(&#39;PC 1&#39;) plt.ylabel(&#39;PC 2&#39;) plt.legend(loc=&#39;lower left&#39;) plt.tight_layout() # plt.savefig(&#39;images/05_03.png&#39;, dpi=300) plt.show() . Using PCA from SKlearn . import seaborn as sns import matplotlib.pyplot as plt import numpy as np . from sklearn.decomposition import PCA # Instantiate PCA without specifying number of components pca_all = PCA() # Fit to scaled data pca_all.fit(X_train_std) # Save cumulative explained variance cum_var = (np.cumsum(pca_all.explained_variance_ratio_)) n_comp = [i for i in range(1, pca_all.n_components_ + 1)] # Plot cumulative variance ax = sns.pointplot(x=n_comp, y=cum_var) ax.set(xlabel=&#39;number of principal components&#39;, ylabel=&#39;cumulative explained variance&#39;) plt.show() . print(y_train) . [3 1 1 1 3 2 2 3 2 2 2 1 2 3 1 3 2 1 3 3 2 1 2 2 2 2 3 1 2 2 1 1 3 1 2 1 1 2 3 3 1 3 3 3 1 2 3 3 2 3 2 2 2 1 2 2 3 3 2 1 1 2 3 3 2 1 2 2 2 1 1 1 1 1 3 1 2 3 2 2 3 1 2 1 2 2 3 2 1 1 1 3 2 1 1 2 2 3 3 2 1 1 2 2 3 1 3 1 2 2 2 2 1 3 1 1 1 1 2 2 3 3 2 2] . from mpl_toolkits.mplot3d import Axes3D # Instantiate PCA with 3 components pca_3 = PCA(3) # Fit to scaled data pca_3.fit(X_train_std) # Transform scaled data data_3pc = pca_3.transform(X_train_std) # Render the 3D plot fig = plt.figure(figsize=(15,15)) ax = fig.add_subplot(111, projection=&#39;3d&#39;) labels = y_train ax.scatter(data_3pc[:, 0], data_3pc[:, 1], data_3pc[:, 2], c=labels, cmap=plt.cm.Set1, edgecolor=&#39;k&#39;, s=25, label=df_wine[&#39;Class label&#39;]) ax.legend([&quot;Class label&quot;], fontsize=&quot;large&quot;) ax.set_title(&quot;First three PCA directions&quot;) ax.set_xlabel(&quot;1st principal component&quot;) ax.w_xaxis.set_ticklabels([]) ax.set_ylabel(&quot;2nd principal component&quot;) ax.w_yaxis.set_ticklabels([]) ax.set_zlabel(&quot;3rd principal component&quot;) ax.w_zaxis.set_ticklabels([]) plt.show() . pca_2 = PCA(2) # Fit and transform scaled data pca_2.fit(X_train_std) data_2pc = pca_2.transform(X_train_std) # Render the 2D plot ax = sns.scatterplot(x=data_2pc[:,0], y=data_2pc[:,1], hue=labels, palette=sns.color_palette(&quot;muted&quot;, n_colors=3)) ax.set(xlabel=&#39;1st principal component&#39;, ylabel=&#39;2nd principal component&#39;, title=&#39;First two PCA directions&#39;) plt.show() .",
            "url": "https://marcelcastrobr.github.io/notebooks/jupyter/dimensionality/pca/2021/09/21/PCA.html",
            "relUrl": "/jupyter/dimensionality/pca/2021/09/21/PCA.html",
            "date": " • Sep 21, 2021"
        }
        
    
  
    
        ,"post7": {
            "title": "Using Keras Tuner for hyperparameter tunning",
            "content": "Download and prepare the dataset . Kaggle competition: Tabular Playground Series - Aug 2021 -https://www.kaggle.com/c/tabular-playground-series-aug-2021 . Dataset: The dataset is used for this competition is synthetic, but based on a real dataset and generated using a CTGAN. The original dataset deals with calculating the loss associated with a loan defaults. Although the features are anonymized, they have properties relating to real-world features. . Target: loss column . Notebook The code of this notebook is inspired by the lab Intro to Keras Tuner from Robert Crowe used on Course Machine Learning Modeling Pipelines in Production by DeepLearning.AI . # Utilities import os import logging # For visualization import matplotlib as mpl import matplotlib.pyplot as plt import pandas as pd import numpy as np # For modelling import tensorflow as tf from tensorflow import feature_column as fc from tensorflow.keras import layers, models from tensorflow import keras from sklearn.model_selection import train_test_split # Set TF logger to only print errors (dismiss warnings) logging.getLogger(&quot;tensorflow&quot;).setLevel(logging.ERROR) . Loading the dataset . import os #for dirname, _, filenames in os.walk(&#39;/kaggle/input&#39;): for dirname, _, filenames in os.walk(&#39;./input&#39;): for filename in filenames: print(os.path.join(dirname, filename)) . if not os.path.isdir(&quot;/tmp/data&quot;): os.makedirs(&quot;/tmp/data&quot;) . import pandas as pd from sklearn.model_selection import train_test_split # Read the data #X = pd.read_csv(&#39;../input/tabular-playground-series-aug-2021/train.csv&#39;, index_col=&#39;id&#39;) #X_test_full = pd.read_csv(&#39;../input/tabular-playground-series-aug-2021/test.csv&#39;, index_col=&#39;id&#39;) Xx = pd.read_csv(&#39;./input/train.csv&#39;, index_col=&#39;id&#39;) #X_test = pd.read_csv(&#39;./input/test.csv&#39;, index_col=&#39;id&#39;) . X = Xx.copy(deep=True) X.dropna(axis=0, subset=[&#39;loss&#39;], inplace=True) #Y = X.loss #X.drop([&#39;loss&#39;], axis=1, inplace=True) . X.describe(include=&#39;all&#39;).transpose() . from sklearn.preprocessing import StandardScaler scaler = StandardScaler() X_scaled = scaler.fit_transform(X) #X_test_index = X_test.index #X_test_scaled = scaler.transform(X_test) . train, valid = train_test_split(X, test_size=0.2, random_state = 123) . print(train.shape) print(valid.shape) . pd.DataFrame(train).to_csv(&quot;/tmp/data/tabular-train.csv&quot;, index=False) pd.DataFrame(valid).to_csv(&quot;/tmp/data/tabular-valid.csv&quot;, index=False) #pd.DataFrame(X_test_scaled).to_csv(&quot;/tmp/data/tabular-test.csv&quot;) . !ls -l /tmp/data/*.csv . pd.read_csv(&#39;/tmp/data/tabular-train.csv&#39;).columns . pd.read_csv(&#39;/tmp/data/tabular-valid.csv&#39;).pop(&#39;loss&#39;).columns . Create input pipeline . LABEL_COLUMN = &#39;loss&#39; # Specify numerical columns # Note you should create another list with STRING_COLS if you # had text data but in this case all features are numerical NUMERIC_COLS = [&#39;f0&#39;, &#39;f1&#39;, &#39;f2&#39;, &#39;f3&#39;, &#39;f4&#39;, &#39;f5&#39;, &#39;f6&#39;] # A function to separate features and labels def features_and_labels(row_data): label = row_data.pop(LABEL_COLUMN) return row_data, label # A utility method to create a tf.data dataset from a CSV file def load_dataset(pattern, batch_size=1, mode=&#39;eval&#39;): dataset = tf.data.experimental.make_csv_dataset(pattern, batch_size) dataset = dataset.map(features_and_labels) # features, label if mode == &#39;train&#39;: # Notice the repeat method is used so this dataset will loop infinitely dataset = dataset.shuffle(1000).repeat() # take advantage of multi-threading; 1=AUTOTUNE dataset = dataset.prefetch(1) return dataset . Building DNN Model . def build_dnn_model(): # input layer inputs = { colname: layers.Input(name=colname, shape=(), dtype=&#39;float32&#39;) for colname in NUMERIC_COLS } # feature_columns feature_columns = { colname: fc.numeric_column(colname) for colname in NUMERIC_COLS } # Constructor for DenseFeatures takes a list of numeric columns # and the resulting tensor takes a dictionary of Input layers dnn_inputs = layers.DenseFeatures(feature_columns.values())(inputs) # two hidden layers of 32 and 8 units, respectively h1 = layers.Dense(32, activation=&#39;relu&#39;, name=&#39;h1&#39;)(dnn_inputs) h2 = layers.Dense(8, activation=&#39;relu&#39;, name=&#39;h2&#39;)(h1) # final output is a linear activation because this is a regression problem output = layers.Dense(1, activation=&#39;linear&#39;, name=&#39;fare&#39;)(h2) # Create model with inputs and output model = models.Model(inputs, output) # compile model (Mean Squared Error is suitable for regression) model.compile(optimizer=&#39;adam&#39;, loss=&#39;mse&#39;, metrics=[ tf.keras.metrics.RootMeanSquaredError(name=&#39;rmse&#39;), &#39;mse&#39; ]) return model . model = build_dnn_model() # Plot the layer architecture and relationship between input features #tf.keras.utils.plot_model(model, &#39;dnn_model.png&#39;, show_shapes=False, rankdir=&#39;LR&#39;) . Training the model . NUM_EPOCHS = 20 TRAIN_BATCH_SIZE = 32 NUM_TRAIN_EXAMPLES = len(pd.read_csv(&#39;/tmp/data/tabular-train.csv&#39;)) NUM_EVAL_EXAMPLES = len(pd.read_csv(&#39;/tmp/data/tabular-valid.csv&#39;)) print(f&quot;training split has {NUM_TRAIN_EXAMPLES} examples n&quot;) print(f&quot;evaluation split has {NUM_EVAL_EXAMPLES} examples n&quot;) . trainds = load_dataset(&#39;/tmp/data/tabular-train*&#39;, TRAIN_BATCH_SIZE, &#39;train&#39;) # Evaluation dataset evalds = load_dataset(&#39;/tmp/data/tabular-valid*&#39;, 1000, &#39;eval&#39;).take(NUM_EVAL_EXAMPLES//1000) # Needs to be specified since the dataset is infinite # This happens because the repeat method was used when creating the dataset steps_per_epoch = NUM_TRAIN_EXAMPLES // TRAIN_BATCH_SIZE # Train the model and save the history history = model.fit(trainds, validation_data=evalds, epochs=NUM_EPOCHS ,steps_per_epoch=steps_per_epoch) . print(type(trainds)) . DESIRED_MAE = 5 class myCallback(tf.keras.callbacks.Callback): def on_epoch_end(self, epoch, logs={}): if(logs.get(&#39;mean_absolute_error&#39;) &lt; DESIRED_MAE): print(&quot; nReached {}% MAE so cancelling training!&quot;.format(DESIRED_MAE)) self.model.stop_training = True . NUM_EPOCHS = 5 . inputs = keras.Input(shape=(X_train.shape[1])) model_dnn = tf.keras.models.Sequential([ tf.keras.layers.Dense(512,activation=&#39;relu&#39;,name=&#39;dense_1&#39;), tf.keras.layers.Dropout(0.2), tf.keras.layers.Dense(10,activation=&#39;relu&#39;,name=&#39;dense_2&#39;), tf.keras.layers.Dropout(0.2), tf.keras.layers.Dense(1,activation=&#39;relu&#39;,name=&#39;dense_3&#39;) ]) model_dnn.compile(loss=&quot;mean_absolute_error&quot;, optimizer=keras.optimizers.Adam(learning_rate=0.001), metrics=[&quot;mean_absolute_error&quot;]) #model_dnn.summary() history = model_dnn.fit(X_train, Y_train,epochs=NUM_EPOCHS, callbacks=[myCallback()]) . model_dnn.summary() . b_eval_dict = model_dnn.evaluate(X_valid, Y_valid, return_dict=True) . Let&#39;s define a helper function for displaying the results so it&#39;s easier to compare later. . def print_results(model, model_name, eval_dict): &#39;&#39;&#39; Prints the values of the hyparameters to tune, and the results of model evaluation Args: model (Model) - Keras model to evaluate model_name (string) - arbitrary string to be used in identifying the model eval_dict (dict) - results of model.evaluate &#39;&#39;&#39; print(f&#39; n{model_name}:&#39;) print(f&#39;number of units in 1st Dense layer: {model.get_layer(&quot;dense_1&quot;).units}&#39;) print(f&#39;learning rate for the optimizer: {model.optimizer.lr.numpy()}&#39;) for key,value in eval_dict.items(): print(f&#39;{key}: {value}&#39;) # Print results for baseline model print_results(model_dnn, &#39;BASELINE MODEL&#39;, b_eval_dict) . That&#39;s it for getting the results for a single set of hyperparameters. Let´s use Keras Tuner by having an API to automatically search for the optimal hyperparameters set. . Keras Tuner . To perform hypertuning with Keras Tuner, you will need to: . Define the model | Select which hyperparameters to tune | Define its search space | Define the search strategy | . Install and import packages . You will start by installing and importing the required packages. . !pip install -q -U keras-tuner . import tensorflow as tf import kerastuner as kt . Define the model . The model you set up for hypertuning is called a hypermodel. When you build this model, you define the hyperparameter search space in addition to the model architecture. . You can define a hypermodel through two approaches: . By using a model builder function | By subclassing the HyperModel class of the Keras Tuner API | . In this lab, you will take the first approach: you will use a model builder function to define the image classification model. This function returns a compiled model and uses hyperparameters you define inline to hypertune the model. . The function below basically builds the same model you used earlier. The difference is there are two hyperparameters that are setup for tuning: . the number of hidden units of the first Dense layer | the learning rate of the Adam optimizer | . You will see that this is done with a HyperParameters object which configures the hyperparameter you&#39;d like to tune. For this exercise, you will: . use its Int() method to define the search space for the Dense units. This allows you to set a minimum and maximum value, as well as the step size when incrementing between these values. . | use its Choice() method for the learning rate. This allows you to define discrete values to include in the search space when hypertuning. . | . You can view all available methods and its sample usage in the official documentation. . def model_builder(hp): &#39;&#39;&#39; Builds the model and sets up the hyperparameters to tune. Args: hp - Keras tuner object Returns: model with hyperparameters to tune &#39;&#39;&#39; # Initialize the Sequential API and start stacking the layers model = keras.Sequential() #model.add(keras.layers.Flatten(input_shape=(28, 28))) # Tune the number of units in the first Dense layer # Choose an optimal value between 32-512 hp_units = hp.Int(&#39;units&#39;, min_value=32, max_value=512, step=32) model.add(keras.layers.Dense(units=hp_units, activation=&#39;relu&#39;, name=&#39;dense_1&#39;)) # Add next layers model.add(keras.layers.Dropout(0.2)) model.add(tf.keras.layers.Dense(10,activation=&#39;relu&#39;,name=&#39;dense_2&#39;)) model.add(keras.layers.Dropout(0.2)) model.add(tf.keras.layers.Dense(1,activation=&#39;relu&#39;,name=&#39;dense_3&#39;)) # Tune the learning rate for the optimizer # Choose an optimal value from 0.01, 0.001, or 0.0001 hp_learning_rate = hp.Choice(&#39;learning_rate&#39;, values=[1e-2, 1e-3, 1e-4]) model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate), loss=&quot;mean_absolute_error&quot;, metrics=[&#39;mean_absolute_error&#39;]) return model . Instantiate the Tuner and perform hypertuning . Now that you have the model builder, you can then define how the tuner can find the optimal set of hyperparameters, also called the search strategy. Keras Tuner has four tuners available with built-in strategies - RandomSearch, Hyperband, BayesianOptimization, and Sklearn. . In this tutorial, you will use the Hyperband tuner. Hyperband is an algorithm specifically developed for hyperparameter optimization. It uses adaptive resource allocation and early-stopping to quickly converge on a high-performing model. This is done using a sports championship style bracket wherein the algorithm trains a large number of models for a few epochs and carries forward only the top-performing half of models to the next round. You can read about the intuition behind the algorithm in section 3 of this paper. . Hyperband determines the number of models to train in a bracket by computing 1 + log`factor`(max_epochs) and rounding it up to the nearest integer. You will see these parameters (i.e. factor and max_epochs passed into the initializer below). In addition, you will also need to define the following to instantiate the Hyperband tuner: . the hypermodel (built by your model builder function) | the objective to optimize (e.g. validation accuracy) | a directory to save logs and checkpoints for every trial (model configuration) run during the hyperparameter search. If you re-run the hyperparameter search, the Keras Tuner uses the existing state from these logs to resume the search. To disable this behavior, pass an additional overwrite=True argument while instantiating the tuner. | the project_name to differentiate with other runs. This will be used as a subdirectory name under the directory. | . You can refer to the documentation for other arguments you can pass in. . tuner = kt.Hyperband(model_builder, objective=&#39;val_mean_absolute_error&#39;, max_epochs=5, factor=3, directory=&#39;kt_dir&#39;, project_name=&#39;kt_hyperband&#39;) . Let&#39;s see a summary of the hyperparameters that you will tune: . tuner.search_space_summary() . You can pass in a callback to stop training early when a metric is not improving. Below, we define an EarlyStopping callback to monitor the validation loss and stop training if it&#39;s not improving after 5 epochs. . stop_early = tf.keras.callbacks.EarlyStopping(monitor=&#39;val_mean_absolute_error&#39;, patience=5) . You will now run the hyperparameter search. The arguments for the search method are the same as those used for tf.keras.model.fit in addition to the callback above. This will take around 10 minutes to run. . tuner.search(X_train, Y_train, epochs=NUM_EPOCHS, validation_split=0.2, callbacks=[stop_early]) . You can get the top performing model with the get_best_hyperparameters() method. . best_hps=tuner.get_best_hyperparameters()[0] print(f&quot;&quot;&quot; The hyperparameter search is complete. The optimal number of units in the first densely-connected layer is {best_hps.get(&#39;units&#39;)} and the optimal learning rate for the optimizer is {best_hps.get(&#39;learning_rate&#39;)}. &quot;&quot;&quot;) . Build and train the model . Now that you have the best set of hyperparameters, you can rebuild the hypermodel with these values and retrain it. . h_model = tuner.hypermodel.build(best_hps) #h_model.summary() . h_model.fit(X_train, Y_train, epochs=NUM_EPOCHS, validation_split=0.2) . You will then get its performance against the test set. . h_eval_dict = h_model.evaluate(X_valid, Y_valid, return_dict=True) . We can compare the results we got with the baseline model we used at the start of the notebook. Results may vary but you will usually get a model that has less units in the dense layer, while having comparable loss and accuracy. This indicates that you reduced the model size and saved compute resources while still having more or less the same accuracy. . #print_results(b_model, &#39;BASELINE MODEL&#39;, b_eval_dict) print_results(h_model, &#39;HYPERTUNED MODEL&#39;, h_eval_dict) . Possible Improvements . If you want to keep practicing with Keras Tuner in this notebook, you can do a factory reset (Runtime &gt; Factory reset runtime) and take on any of the following: . hypertune the dropout layer with hp.Float() or hp.Choice() | hypertune the activation function of the 1st dense layer with hp.Choice() | determine the optimal number of Dense layers you can add to improve the model. You can use the code here as reference. | explore pre-defined HyperModel classes - HyperXception and HyperResNet for computer vision applications. | . Wrap Up . In this tutorial, you used Keras Tuner to conveniently tune hyperparameters. You defined which ones to tune, the search space, and search strategy to arrive at the optimal set of hyperparameters. These concepts will again be discussed in the next sections but in the context of AutoML, a package that automates the entire machine learning pipeline. On to the next! .",
            "url": "https://marcelcastrobr.github.io/notebooks/kerastuner/tensorflow/2021/09/05/KerasTuner.html",
            "relUrl": "/kerastuner/tensorflow/2021/09/05/KerasTuner.html",
            "date": " • Sep 5, 2021"
        }
        
    
  
    
        ,"post8": {
            "title": "Feature Engineering with Tensorflow",
            "content": "Week 1 Assignment: Data Validation . Tensorflow Data Validation (TFDV) is an open-source library that helps to understand, validate, and monitor production machine learning (ML) data at scale. Common use-cases include comparing training, evaluation and serving datasets, as well as checking for training/serving skew. You have seen the core functionalities of this package in the previous ungraded lab and you will get to practice them in this week&#39;s assignment. . In this lab, you will use TFDV in order to: . Generate and visualize statistics from a dataframe | Infer a dataset schema | Calculate, visualize and fix anomalies | . Let&#39;s begin! . . 1 - Setup and Imports . import os import pandas as pd import tensorflow as tf import tempfile, urllib, zipfile import tensorflow_data_validation as tfdv from tensorflow.python.lib.io import file_io from tensorflow_data_validation.utils import slicing_util from tensorflow_metadata.proto.v0.statistics_pb2 import DatasetFeatureStatisticsList, DatasetFeatureStatistics # Set TF&#39;s logger to only display errors to avoid internal warnings being shown tf.get_logger().setLevel(&#39;ERROR&#39;) . . 2 - Load the Dataset . You will be using the Diabetes 130-US hospitals for years 1999-2008 Data Set donated to the University of California, Irvine (UCI) Machine Learning Repository. The dataset represents 10 years (1999-2008) of clinical care at 130 US hospitals and integrated delivery networks. It includes over 50 features representing patient and hospital outcomes. . This dataset has already been included in your Jupyter workspace so you can easily load it. . . 2.1 Read and Split the Dataset . df = pd.read_csv(&#39;dataset_diabetes/diabetic_data.csv&#39;, header=0, na_values = &#39;?&#39;) # Preview the dataset df.head() . encounter_id patient_nbr race gender age weight admission_type_id discharge_disposition_id admission_source_id time_in_hospital ... citoglipton insulin glyburide-metformin glipizide-metformin glimepiride-pioglitazone metformin-rosiglitazone metformin-pioglitazone change diabetesMed readmitted . 0 2278392 | 8222157 | Caucasian | Female | [0-10) | NaN | 6 | 25 | 1 | 1 | ... | No | No | No | No | No | No | No | No | No | NO | . 1 149190 | 55629189 | Caucasian | Female | [10-20) | NaN | 1 | 1 | 7 | 3 | ... | No | Up | No | No | No | No | No | Ch | Yes | &gt;30 | . 2 64410 | 86047875 | AfricanAmerican | Female | [20-30) | NaN | 1 | 1 | 7 | 2 | ... | No | No | No | No | No | No | No | No | Yes | NO | . 3 500364 | 82442376 | Caucasian | Male | [30-40) | NaN | 1 | 1 | 7 | 2 | ... | No | Up | No | No | No | No | No | Ch | Yes | NO | . 4 16680 | 42519267 | Caucasian | Male | [40-50) | NaN | 1 | 1 | 7 | 1 | ... | No | Steady | No | No | No | No | No | Ch | Yes | NO | . 5 rows × 50 columns . . Data splits . In a production ML system, the model performance can be negatively affected by anomalies and divergence between data splits for training, evaluation, and serving. To emulate a production system, you will split the dataset into: . 70% training set | 15% evaluation set | 15% serving set | . You will then use TFDV to visualize, analyze, and understand the data. You will create a data schema from the training dataset, then compare the evaluation and serving sets with this schema to detect anomalies and data drift/skew. . . Label Column . This dataset has been prepared to analyze the factors related to readmission outcome. In this notebook, you will treat the readmitted column as the target or label column. . The target (or label) is important to know while splitting the data into training, evaluation and serving sets. In supervised learning, you need to include the target in the training and evaluation datasets. For the serving set however (i.e. the set that simulates the data coming from your users), the label column needs to be dropped since that is the feature that your model will be trying to predict. . The following function returns the training, evaluation and serving partitions of a given dataset: . def prepare_data_splits_from_dataframe(df): &#39;&#39;&#39; Splits a Pandas Dataframe into training, evaluation and serving sets. Parameters: df : pandas dataframe to split Returns: train_df: Training dataframe(70% of the entire dataset) eval_df: Evaluation dataframe (15% of the entire dataset) serving_df: Serving dataframe (15% of the entire dataset, label column dropped) &#39;&#39;&#39; # 70% of records for generating the training set train_len = int(len(df) * 0.7) # Remaining 30% of records for generating the evaluation and serving sets eval_serv_len = len(df) - train_len # Half of the 30%, which makes up 15% of total records, for generating the evaluation set eval_len = eval_serv_len // 2 # Remaining 15% of total records for generating the serving set serv_len = eval_serv_len - eval_len # Sample the train, validation and serving sets. We specify a random state for repeatable outcomes. train_df = df.iloc[:train_len].sample(frac=1, random_state=48).reset_index(drop=True) eval_df = df.iloc[train_len: train_len + eval_len].sample(frac=1, random_state=48).reset_index(drop=True) serving_df = df.iloc[train_len + eval_len: train_len + eval_len + serv_len].sample(frac=1, random_state=48).reset_index(drop=True) # Serving data emulates the data that would be submitted for predictions, so it should not have the label column. serving_df = serving_df.drop([&#39;readmitted&#39;], axis=1) return train_df, eval_df, serving_df . train_df, eval_df, serving_df = prepare_data_splits_from_dataframe(df) print(&#39;Training dataset has {} records nValidation dataset has {} records nServing dataset has {} records&#39;.format(len(train_df),len(eval_df),len(serving_df))) . Training dataset has 71236 records Validation dataset has 15265 records Serving dataset has 15265 records . . 3 - Generate and Visualize Training Data Statistics . In this section, you will be generating descriptive statistics from the dataset. This is usually the first step when dealing with a dataset you are not yet familiar with. It is also known as performing an exploratory data analysis and its purpose is to understand the data types, the data itself and any possible issues that need to be addressed. . It is important to mention that exploratory data analysis should be perfomed on the training dataset only. This is because getting information out of the evaluation or serving datasets can be seen as &quot;cheating&quot; since this data is used to emulate data that you have not collected yet and will try to predict using your ML algorithm. In general, it is a good practice to avoid leaking information from your evaluation and serving data into your model. . . Removing Irrelevant Features . Before you generate the statistics, you may want to drop irrelevant features from your dataset. You can do that with TFDV with the tfdv.StatsOptions class. It is usually not a good idea to drop features without knowing what information they contain. However there are times when this can be fairly obvious. . One of the important parameters of the StatsOptions class is feature_whitelist, which defines the features to include while calculating the data statistics. You can check the documentation to learn more about the class arguments. . In this case, you will omit the statistics for encounter_id and patient_nbr since they are part of the internal tracking of patients in the hospital and they don&#39;t contain valuable information for the task at hand. . features_to_remove = {&#39;encounter_id&#39;, &#39;patient_nbr&#39;} # Collect features to whitelist while computing the statistics approved_cols = [col for col in df.columns if (col not in features_to_remove)] # Instantiate a StatsOptions class and define the feature_whitelist property stats_options = tfdv.StatsOptions(feature_whitelist=approved_cols) # Review the features to generate the statistics print(stats_options.feature_whitelist) . [&#39;race&#39;, &#39;gender&#39;, &#39;age&#39;, &#39;weight&#39;, &#39;admission_type_id&#39;, &#39;discharge_disposition_id&#39;, &#39;admission_source_id&#39;, &#39;time_in_hospital&#39;, &#39;payer_code&#39;, &#39;medical_specialty&#39;, &#39;num_lab_procedures&#39;, &#39;num_procedures&#39;, &#39;num_medications&#39;, &#39;number_outpatient&#39;, &#39;number_emergency&#39;, &#39;number_inpatient&#39;, &#39;diag_1&#39;, &#39;diag_2&#39;, &#39;diag_3&#39;, &#39;number_diagnoses&#39;, &#39;max_glu_serum&#39;, &#39;A1Cresult&#39;, &#39;metformin&#39;, &#39;repaglinide&#39;, &#39;nateglinide&#39;, &#39;chlorpropamide&#39;, &#39;glimepiride&#39;, &#39;acetohexamide&#39;, &#39;glipizide&#39;, &#39;glyburide&#39;, &#39;tolbutamide&#39;, &#39;pioglitazone&#39;, &#39;rosiglitazone&#39;, &#39;acarbose&#39;, &#39;miglitol&#39;, &#39;troglitazone&#39;, &#39;tolazamide&#39;, &#39;examide&#39;, &#39;citoglipton&#39;, &#39;insulin&#39;, &#39;glyburide-metformin&#39;, &#39;glipizide-metformin&#39;, &#39;glimepiride-pioglitazone&#39;, &#39;metformin-rosiglitazone&#39;, &#39;metformin-pioglitazone&#39;, &#39;change&#39;, &#39;diabetesMed&#39;, &#39;readmitted&#39;] . . Exercise 1: Generate Training Statistics . TFDV allows you to generate statistics from different data formats such as CSV or a Pandas DataFrame. . Since you already have the data stored in a DataFrame you can use the function tfdv.generate_statistics_from_dataframe() which, given a DataFrame and stats_options, generates an object of type DatasetFeatureStatisticsList. This object includes the computed statistics of the given dataset. . Complete the cell below to generate the statistics of the training set. Remember to pass the training dataframe and the stats_options that you defined above as arguments. . train_stats = tfdv.generate_statistics_from_dataframe(train_df, stats_options) ### END CODE HERE . # get the number of features used to compute statistics print(f&quot;Number of features used: {len(train_stats.datasets[0].features)}&quot;) # check the number of examples used print(f&quot;Number of examples used: {train_stats.datasets[0].num_examples}&quot;) # check the column names of the first and last feature print(f&quot;First feature: {train_stats.datasets[0].features[0].path.step[0]}&quot;) print(f&quot;Last feature: {train_stats.datasets[0].features[-1].path.step[0]}&quot;) . Number of features used: 48 Number of examples used: 71236 First feature: race Last feature: readmitted . Expected Output: . Number of features used: 48 Number of examples used: 71236 First feature: race Last feature: readmitted . . Exercise 2: Visualize Training Statistics . Now that you have the computed statistics in the DatasetFeatureStatisticsList instance, you will need a way to visualize these to get actual insights. TFDV provides this functionality through the method tfdv.visualize_statistics(). . Using this function in an interactive Python environment such as this one will output a very nice and convenient way to interact with the descriptive statistics you generated earlier. . Try it out yourself! Remember to pass in the generated training statistics in the previous exercise as an argument. . tfdv.visualize_statistics(train_stats) ### END CODE HERE . . 4 - Infer a data schema . A schema defines the properties of the data and can thus be used to detect errors. Some of these properties include: . which features are expected to be present | feature type | the number of values for a feature in each example | the presence of each feature across all examples | the expected domains of features | . The schema is expected to be fairly static, whereas statistics can vary per data split. So, you will infer the data schema from only the training dataset. Later, you will generate statistics for evaluation and serving datasets and compare their state with the data schema to detect anomalies, drift and skew. . . Exercise 3: Infer the training set schema . Schema inference is straightforward using tfdv.infer_schema(). This function needs only the statistics (an instance of DatasetFeatureStatisticsList) of your data as input. The output will be a Schema protocol buffer containing the results. . A complimentary function is tfdv.display_schema() for displaying the schema in a table. This accepts a Schema protocol buffer as input. . Fill the code below to infer the schema from the training statistics using TFDV and display the result. . # Infer the data schema by using the training statistics that you generated schema = tfdv.infer_schema(statistics=train_stats) # Display the data schema tfdv.display_schema(schema) ### END CODE HERE . Type Presence Valency Domain . Feature name . &#39;race&#39; STRING | optional | single | &#39;race&#39; | . &#39;gender&#39; STRING | required | | &#39;gender&#39; | . &#39;age&#39; STRING | required | | &#39;age&#39; | . &#39;weight&#39; STRING | optional | single | &#39;weight&#39; | . &#39;admission_type_id&#39; INT | required | | - | . &#39;discharge_disposition_id&#39; INT | required | | - | . &#39;admission_source_id&#39; INT | required | | - | . &#39;time_in_hospital&#39; INT | required | | - | . &#39;payer_code&#39; STRING | optional | single | &#39;payer_code&#39; | . &#39;medical_specialty&#39; STRING | optional | single | &#39;medical_specialty&#39; | . &#39;num_lab_procedures&#39; INT | required | | - | . &#39;num_procedures&#39; INT | required | | - | . &#39;num_medications&#39; INT | required | | - | . &#39;number_outpatient&#39; INT | required | | - | . &#39;number_emergency&#39; INT | required | | - | . &#39;number_inpatient&#39; INT | required | | - | . &#39;diag_1&#39; BYTES | optional | single | - | . &#39;diag_2&#39; BYTES | optional | single | - | . &#39;diag_3&#39; BYTES | optional | single | - | . &#39;number_diagnoses&#39; INT | required | | - | . &#39;max_glu_serum&#39; STRING | required | | &#39;max_glu_serum&#39; | . &#39;A1Cresult&#39; STRING | required | | &#39;A1Cresult&#39; | . &#39;metformin&#39; STRING | required | | &#39;metformin&#39; | . &#39;repaglinide&#39; STRING | required | | &#39;repaglinide&#39; | . &#39;nateglinide&#39; STRING | required | | &#39;nateglinide&#39; | . &#39;chlorpropamide&#39; STRING | required | | &#39;chlorpropamide&#39; | . &#39;glimepiride&#39; STRING | required | | &#39;glimepiride&#39; | . &#39;acetohexamide&#39; STRING | required | | &#39;acetohexamide&#39; | . &#39;glipizide&#39; STRING | required | | &#39;glipizide&#39; | . &#39;glyburide&#39; STRING | required | | &#39;glyburide&#39; | . &#39;tolbutamide&#39; STRING | required | | &#39;tolbutamide&#39; | . &#39;pioglitazone&#39; STRING | required | | &#39;pioglitazone&#39; | . &#39;rosiglitazone&#39; STRING | required | | &#39;rosiglitazone&#39; | . &#39;acarbose&#39; STRING | required | | &#39;acarbose&#39; | . &#39;miglitol&#39; STRING | required | | &#39;miglitol&#39; | . &#39;troglitazone&#39; STRING | required | | &#39;troglitazone&#39; | . &#39;tolazamide&#39; STRING | required | | &#39;tolazamide&#39; | . &#39;examide&#39; STRING | required | | &#39;examide&#39; | . &#39;citoglipton&#39; STRING | required | | &#39;citoglipton&#39; | . &#39;insulin&#39; STRING | required | | &#39;insulin&#39; | . &#39;glyburide-metformin&#39; STRING | required | | &#39;glyburide-metformin&#39; | . &#39;glipizide-metformin&#39; STRING | required | | &#39;glipizide-metformin&#39; | . &#39;glimepiride-pioglitazone&#39; STRING | required | | &#39;glimepiride-pioglitazone&#39; | . &#39;metformin-rosiglitazone&#39; STRING | required | | &#39;metformin-rosiglitazone&#39; | . &#39;metformin-pioglitazone&#39; STRING | required | | &#39;metformin-pioglitazone&#39; | . &#39;change&#39; STRING | required | | &#39;change&#39; | . &#39;diabetesMed&#39; STRING | required | | &#39;diabetesMed&#39; | . &#39;readmitted&#39; STRING | required | | &#39;readmitted&#39; | . Values . Domain . &#39;race&#39; &#39;AfricanAmerican&#39;, &#39;Asian&#39;, &#39;Caucasian&#39;, &#39;Hispanic&#39;, &#39;Other&#39; | . &#39;gender&#39; &#39;Female&#39;, &#39;Male&#39;, &#39;Unknown/Invalid&#39; | . &#39;age&#39; &#39;[0-10)&#39;, &#39;[10-20)&#39;, &#39;[20-30)&#39;, &#39;[30-40)&#39;, &#39;[40-50)&#39;, &#39;[50-60)&#39;, &#39;[60-70)&#39;, &#39;[70-80)&#39;, &#39;[80-90)&#39;, &#39;[90-100)&#39; | . &#39;weight&#39; &#39;&gt;200&#39;, &#39;[0-25)&#39;, &#39;[100-125)&#39;, &#39;[125-150)&#39;, &#39;[150-175)&#39;, &#39;[175-200)&#39;, &#39;[25-50)&#39;, &#39;[50-75)&#39;, &#39;[75-100)&#39; | . &#39;payer_code&#39; &#39;BC&#39;, &#39;CH&#39;, &#39;CM&#39;, &#39;CP&#39;, &#39;DM&#39;, &#39;HM&#39;, &#39;MC&#39;, &#39;MD&#39;, &#39;MP&#39;, &#39;OG&#39;, &#39;OT&#39;, &#39;PO&#39;, &#39;SI&#39;, &#39;SP&#39;, &#39;UN&#39;, &#39;WC&#39; | . &#39;medical_specialty&#39; &#39;AllergyandImmunology&#39;, &#39;Anesthesiology&#39;, &#39;Anesthesiology-Pediatric&#39;, &#39;Cardiology&#39;, &#39;Cardiology-Pediatric&#39;, &#39;Dentistry&#39;, &#39;Dermatology&#39;, &#39;Emergency/Trauma&#39;, &#39;Endocrinology&#39;, &#39;Family/GeneralPractice&#39;, &#39;Gastroenterology&#39;, &#39;Gynecology&#39;, &#39;Hematology&#39;, &#39;Hematology/Oncology&#39;, &#39;Hospitalist&#39;, &#39;InfectiousDiseases&#39;, &#39;InternalMedicine&#39;, &#39;Nephrology&#39;, &#39;Neurology&#39;, &#39;Obsterics&amp;Gynecology-GynecologicOnco&#39;, &#39;Obstetrics&#39;, &#39;ObstetricsandGynecology&#39;, &#39;Oncology&#39;, &#39;Ophthalmology&#39;, &#39;Orthopedics&#39;, &#39;Orthopedics-Reconstructive&#39;, &#39;Osteopath&#39;, &#39;Otolaryngology&#39;, &#39;OutreachServices&#39;, &#39;Pathology&#39;, &#39;Pediatrics&#39;, &#39;Pediatrics-AllergyandImmunology&#39;, &#39;Pediatrics-CriticalCare&#39;, &#39;Pediatrics-EmergencyMedicine&#39;, &#39;Pediatrics-Endocrinology&#39;, &#39;Pediatrics-Hematology-Oncology&#39;, &#39;Pediatrics-InfectiousDiseases&#39;, &#39;Pediatrics-Neurology&#39;, &#39;Pediatrics-Pulmonology&#39;, &#39;Perinatology&#39;, &#39;PhysicalMedicineandRehabilitation&#39;, &#39;PhysicianNotFound&#39;, &#39;Podiatry&#39;, &#39;Proctology&#39;, &#39;Psychiatry&#39;, &#39;Psychiatry-Addictive&#39;, &#39;Psychiatry-Child/Adolescent&#39;, &#39;Psychology&#39;, &#39;Pulmonology&#39;, &#39;Radiologist&#39;, &#39;Radiology&#39;, &#39;Rheumatology&#39;, &#39;Speech&#39;, &#39;SportsMedicine&#39;, &#39;Surgeon&#39;, &#39;Surgery-Cardiovascular&#39;, &#39;Surgery-Cardiovascular/Thoracic&#39;, &#39;Surgery-Colon&amp;Rectal&#39;, &#39;Surgery-General&#39;, &#39;Surgery-Maxillofacial&#39;, &#39;Surgery-Neuro&#39;, &#39;Surgery-Pediatric&#39;, &#39;Surgery-Plastic&#39;, &#39;Surgery-PlasticwithinHeadandNeck&#39;, &#39;Surgery-Thoracic&#39;, &#39;Surgery-Vascular&#39;, &#39;SurgicalSpecialty&#39;, &#39;Urology&#39; | . &#39;max_glu_serum&#39; &#39;&gt;200&#39;, &#39;&gt;300&#39;, &#39;None&#39;, &#39;Norm&#39; | . &#39;A1Cresult&#39; &#39;&gt;7&#39;, &#39;&gt;8&#39;, &#39;None&#39;, &#39;Norm&#39; | . &#39;metformin&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;repaglinide&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;nateglinide&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;chlorpropamide&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;glimepiride&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;acetohexamide&#39; &#39;No&#39;, &#39;Steady&#39; | . &#39;glipizide&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;glyburide&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;tolbutamide&#39; &#39;No&#39;, &#39;Steady&#39; | . &#39;pioglitazone&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;rosiglitazone&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;acarbose&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;miglitol&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;troglitazone&#39; &#39;No&#39;, &#39;Steady&#39; | . &#39;tolazamide&#39; &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;examide&#39; &#39;No&#39; | . &#39;citoglipton&#39; &#39;No&#39; | . &#39;insulin&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;glyburide-metformin&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;glipizide-metformin&#39; &#39;No&#39;, &#39;Steady&#39; | . &#39;glimepiride-pioglitazone&#39; &#39;No&#39; | . &#39;metformin-rosiglitazone&#39; &#39;No&#39; | . &#39;metformin-pioglitazone&#39; &#39;No&#39; | . &#39;change&#39; &#39;Ch&#39;, &#39;No&#39; | . &#39;diabetesMed&#39; &#39;No&#39;, &#39;Yes&#39; | . &#39;readmitted&#39; &#39;&lt;30&#39;, &#39;&gt;30&#39;, &#39;NO&#39; | . # Check number of features print(f&quot;Number of features in schema: {len(schema.feature)}&quot;) # Check domain name of 2nd feature print(f&quot;Second feature in schema: {list(schema.feature)[1].domain}&quot;) . Number of features in schema: 48 Second feature in schema: gender . Expected Output: . Number of features in schema: 48 Second feature in schema: gender . Be sure to check the information displayed before moving forward. . . 5 - Calculate, Visualize and Fix Evaluation Anomalies . It is important that the schema of the evaluation data is consistent with the training data since the data that your model is going to receive should be consistent to the one you used to train it with. . Moreover, it is also important that the features of the evaluation data belong roughly to the same range as the training data. This ensures that the model will be evaluated on a similar loss surface covered during training. . . Exercise 4: Compare Training and Evaluation Statistics . Now you are going to generate the evaluation statistics and compare it with training statistics. You can use the tfdv.generate_statistics_from_dataframe() function for this. But this time, you&#39;ll need to pass the evaluation data. For the stats_options parameter, the list you used before works here too. . Remember that to visualize the evaluation statistics you can use tfdv.visualize_statistics(). . However, it is impractical to visualize both statistics separately and do your comparison from there. Fortunately, TFDV has got this covered. You can use the visualize_statistics function and pass additional parameters to overlay the statistics from both datasets (referenced as left-hand side and right-hand side statistics). Let&#39;s see what these parameters are: . lhs_statistics: Required parameter. Expects an instance of DatasetFeatureStatisticsList. | . rhs_statistics: Expects an instance of DatasetFeatureStatisticsList to compare with lhs_statistics. | . lhs_name: Name of the lhs_statistics dataset. | . rhs_name: Name of the rhs_statistics dataset. | . For this case, remember to define the lhs_statistics protocol with the eval_stats, and the optional rhs_statistics protocol with the train_stats. . Additionally, check the function for the protocol name declaration, and define the lhs and rhs names as &#39;EVAL_DATASET&#39; and &#39;TRAIN_DATASET&#39; respectively. . # Generate evaluation dataset statistics # HINT: Remember to use the evaluation dataframe and to pass the stats_options (that you defined before) as an argument eval_stats = tfdv.generate_statistics_from_dataframe(eval_df, stats_options) # Compare evaluation data with training data # HINT: Remember to use both the evaluation and training statistics with the lhs_statistics and rhs_statistics arguments # HINT: Assign the names of &#39;EVAL_DATASET&#39; and &#39;TRAIN_DATASET&#39; to the lhs and rhs protocols tfdv.visualize_statistics( lhs_statistics=eval_stats, rhs_statistics=train_stats, lhs_name=&#39;EVAL_DATASET&#39;, rhs_name=&#39;TRAIN_DATASET&#39; ) ### END CODE HERE . # get the number of features used to compute statistics print(f&quot;Number of features: {len(eval_stats.datasets[0].features)}&quot;) # check the number of examples used print(f&quot;Number of examples: {eval_stats.datasets[0].num_examples}&quot;) # check the column names of the first and last feature print(f&quot;First feature: {eval_stats.datasets[0].features[0].path.step[0]}&quot;) print(f&quot;Last feature: {eval_stats.datasets[0].features[-1].path.step[0]}&quot;) . Number of features: 48 Number of examples: 15265 First feature: race Last feature: readmitted . Expected Output: . Number of features: 48 Number of examples: 15265 First feature: race Last feature: readmitted . . Exercise 5: Detecting Anomalies . At this point, you should ask if your evaluation dataset matches the schema from your training dataset. For instance, if you scroll through the output cell in the previous exercise, you can see that the categorical feature glimepiride-pioglitazone has 1 unique value in the training set while the evaluation dataset has 2. You can verify with the built-in Pandas describe() method as well. . train_df[&quot;glimepiride-pioglitazone&quot;].describe() . count 71236 unique 1 top No freq 71236 Name: glimepiride-pioglitazone, dtype: object . eval_df[&quot;glimepiride-pioglitazone&quot;].describe() . count 15265 unique 2 top No freq 15264 Name: glimepiride-pioglitazone, dtype: object . It is possible but highly inefficient to visually inspect and determine all the anomalies. So, let&#39;s instead use TFDV functions to detect and display these. . You can use the function tfdv.validate_statistics() for detecting anomalies and tfdv.display_anomalies() for displaying them. . The validate_statistics() method has two required arguments: . an instance of DatasetFeatureStatisticsList | an instance of Schema | . Fill in the following graded function which, given the statistics and schema, displays the anomalies found. . def calculate_and_display_anomalies(statistics, schema): &#39;&#39;&#39; Calculate and display anomalies. Parameters: statistics : Data statistics in statistics_pb2.DatasetFeatureStatisticsList format schema : Data schema in schema_pb2.Schema format Returns: display of calculated anomalies &#39;&#39;&#39; ### START CODE HERE # HINTS: Pass the statistics and schema parameters into the validation function anomalies = tfdv.validate_statistics(statistics=statistics, schema=schema) # HINTS: Display input anomalies by using the calculated anomalies tfdv.display_anomalies(anomalies) ### END CODE HERE . You should see detected anomalies in the medical_specialty and glimepiride-pioglitazone features by running the cell below. . calculate_and_display_anomalies(eval_stats, schema=schema) . No anomalies found. . . Exercise 6: Fix evaluation anomalies in the schema . The evaluation data has records with values for the features glimepiride-pioglitazone and medical_speciality that were not included in the schema generated from the training data. You can fix this by adding the new values that exist in the evaluation dataset to the domain of these features. . To get the domain of a particular feature you can use tfdv.get_domain(). . You can use the append() method to the value property of the returned domain to add strings to the valid list of values. To be more explicit, given a domain you can do something like: . domain.value.append(&quot;feature_value&quot;) . # Get the domain associated with the input feature, glimepiride-pioglitazone, from the schema glimepiride_pioglitazone_domain = tfdv.get_domain(schema, &#39;glimepiride-pioglitazone&#39;) print(glimepiride_pioglitazone_domain) # HINT: Append the missing value &#39;Steady&#39; to the domain glimepiride_pioglitazone_domain.value.append(&#39;Steady&#39;) print(glimepiride_pioglitazone_domain) # Get the domain associated with the input feature, medical_specialty, from the schema medical_specialty_domain = tfdv.get_domain(schema, &#39;medical_specialty&#39;) print(type(medical_specialty_domain)) # HINT: Append the missing value &#39;Neurophysiology&#39; to the domain medical_specialty_domain.value.append(&#39;Neurophysiology&#39;) #print(medical_specialty_domain) # HINT: Re-calculate and re-display anomalies with the new schema calculate_and_display_anomalies(eval_stats, schema=schema) ### END CODE HERE . name: &#34;glimepiride-pioglitazone&#34; value: &#34;No&#34; value: &#34;Steady&#34; value: &#34;Steady&#34; value: &#34;Steady&#34; value: &#34;Steady&#34; value: &#34;Steady&#34; name: &#34;glimepiride-pioglitazone&#34; value: &#34;No&#34; value: &#34;Steady&#34; value: &#34;Steady&#34; value: &#34;Steady&#34; value: &#34;Steady&#34; value: &#34;Steady&#34; value: &#34;Steady&#34; &lt;class &#39;tensorflow_metadata.proto.v0.schema_pb2.StringDomain&#39;&gt; . No anomalies found. . If you did the exercise correctly, you should see &quot;No anomalies found.&quot; after running the cell above. . . 6 - Schema Environments . By default, all datasets in a pipeline should use the same schema. However, there are some exceptions. . For example, the label column is dropped in the serving set so this will be flagged when comparing with the training set schema. . In this case, introducing slight schema variations is necessary. . . Exercise 7: Check anomalies in the serving set . Now you are going to check for anomalies in the serving data. The process is very similar to the one you previously did for the evaluation data with a little change. . Let&#39;s create a new StatsOptions that is aware of the information provided by the schema and use it when generating statistics from the serving DataFrame. . options = tfdv.StatsOptions(schema=schema, infer_type_from_schema=True, feature_whitelist=approved_cols) . # Generate serving dataset statistics # HINT: Remember to use the serving dataframe and to pass the newly defined statistics options #serving_stats = tfdv.generate_statistics_from_dataframe(None, stats_options=None) serving_stats = tfdv.generate_statistics_from_dataframe(serving_df, stats_options=options) # HINT: Calculate and display anomalies using the generated serving statistics #calculate_and_display_anomalies(None, schema=None) calculate_and_display_anomalies(serving_stats, schema=schema) ### END CODE HERE . Anomaly short description Anomaly long description . Feature name . &#39;metformin-pioglitazone&#39; Unexpected string values | Examples contain values missing from the schema: Steady (&lt;1%). | . &#39;payer_code&#39; Unexpected string values | Examples contain values missing from the schema: FR (&lt;1%). | . &#39;medical_specialty&#39; Unexpected string values | Examples contain values missing from the schema: DCPTEAM (&lt;1%), Endocrinology-Metabolism (&lt;1%), Resident (&lt;1%). | . &#39;metformin-rosiglitazone&#39; Unexpected string values | Examples contain values missing from the schema: Steady (&lt;1%). | . &#39;readmitted&#39; Column dropped | Column is completely missing | . You should see that metformin-rosiglitazone, metformin-pioglitazone, payer_code and medical_specialty features have an anomaly (i.e. Unexpected string values) which is less than 1%. . Let&#39;s relax the anomaly detection constraints for the last two of these features by defining the min_domain_mass of the feature&#39;s distribution constraints. . # Get the feature and relax to match 90% of the domain payer_code = tfdv.get_feature(schema, &#39;payer_code&#39;) payer_code.distribution_constraints.min_domain_mass = 0.9 # Get the feature and relax to match 90% of the domain medical_specialty = tfdv.get_feature(schema, &#39;medical_specialty&#39;) medical_specialty.distribution_constraints.min_domain_mass = 0.9 # Detect anomalies with the updated constraints calculate_and_display_anomalies(serving_stats, schema=schema) . Anomaly short description Anomaly long description . Feature name . &#39;metformin-pioglitazone&#39; Unexpected string values | Examples contain values missing from the schema: Steady (&lt;1%). | . &#39;metformin-rosiglitazone&#39; Unexpected string values | Examples contain values missing from the schema: Steady (&lt;1%). | . &#39;readmitted&#39; Column dropped | Column is completely missing | . If the payer_code and medical_specialty are no longer part of the output cell, then the relaxation worked! . . Exercise 8: Modifying the Domain . Let&#39;s investigate the possible cause of the anomalies for the other features, namely metformin-pioglitazone and metformin-rosiglitazone. From the output of the previous exercise, you&#39;ll see that the anomaly long description says: &quot;Examples contain values missing from the schema: Steady (&lt;1%)&quot;. You can redisplay the schema and look at the domain of these features to verify this statement. . When you inferred the schema at the start of this lab, it&#39;s possible that some values were not detected in the training data so it was not included in the expected domain values of the feature&#39;s schema. In the case of metformin-rosiglitazone and metformin-pioglitazone, the value &quot;Steady&quot; is indeed missing. You will just see &quot;No&quot; in the domain of these two features after running the code cell below. . tfdv.display_schema(schema) . Type Presence Valency Domain . Feature name . &#39;race&#39; STRING | optional | single | &#39;race&#39; | . &#39;gender&#39; STRING | required | | &#39;gender&#39; | . &#39;age&#39; STRING | required | | &#39;age&#39; | . &#39;weight&#39; STRING | optional | single | &#39;weight&#39; | . &#39;admission_type_id&#39; INT | required | | - | . &#39;discharge_disposition_id&#39; INT | required | | - | . &#39;admission_source_id&#39; INT | required | | - | . &#39;time_in_hospital&#39; INT | required | | - | . &#39;payer_code&#39; STRING | optional | single | &#39;payer_code&#39; | . &#39;medical_specialty&#39; STRING | optional | single | &#39;medical_specialty&#39; | . &#39;num_lab_procedures&#39; INT | required | | - | . &#39;num_procedures&#39; INT | required | | - | . &#39;num_medications&#39; INT | required | | - | . &#39;number_outpatient&#39; INT | required | | - | . &#39;number_emergency&#39; INT | required | | - | . &#39;number_inpatient&#39; INT | required | | - | . &#39;diag_1&#39; BYTES | optional | single | - | . &#39;diag_2&#39; BYTES | optional | single | - | . &#39;diag_3&#39; BYTES | optional | single | - | . &#39;number_diagnoses&#39; INT | required | | - | . &#39;max_glu_serum&#39; STRING | required | | &#39;max_glu_serum&#39; | . &#39;A1Cresult&#39; STRING | required | | &#39;A1Cresult&#39; | . &#39;metformin&#39; STRING | required | | &#39;metformin&#39; | . &#39;repaglinide&#39; STRING | required | | &#39;repaglinide&#39; | . &#39;nateglinide&#39; STRING | required | | &#39;nateglinide&#39; | . &#39;chlorpropamide&#39; STRING | required | | &#39;chlorpropamide&#39; | . &#39;glimepiride&#39; STRING | required | | &#39;glimepiride&#39; | . &#39;acetohexamide&#39; STRING | required | | &#39;acetohexamide&#39; | . &#39;glipizide&#39; STRING | required | | &#39;glipizide&#39; | . &#39;glyburide&#39; STRING | required | | &#39;glyburide&#39; | . &#39;tolbutamide&#39; STRING | required | | &#39;tolbutamide&#39; | . &#39;pioglitazone&#39; STRING | required | | &#39;pioglitazone&#39; | . &#39;rosiglitazone&#39; STRING | required | | &#39;rosiglitazone&#39; | . &#39;acarbose&#39; STRING | required | | &#39;acarbose&#39; | . &#39;miglitol&#39; STRING | required | | &#39;miglitol&#39; | . &#39;troglitazone&#39; STRING | required | | &#39;troglitazone&#39; | . &#39;tolazamide&#39; STRING | required | | &#39;tolazamide&#39; | . &#39;examide&#39; STRING | required | | &#39;examide&#39; | . &#39;citoglipton&#39; STRING | required | | &#39;citoglipton&#39; | . &#39;insulin&#39; STRING | required | | &#39;insulin&#39; | . &#39;glyburide-metformin&#39; STRING | required | | &#39;glyburide-metformin&#39; | . &#39;glipizide-metformin&#39; STRING | required | | &#39;glipizide-metformin&#39; | . &#39;glimepiride-pioglitazone&#39; STRING | required | | &#39;glimepiride-pioglitazone&#39; | . &#39;metformin-rosiglitazone&#39; STRING | required | | &#39;metformin-rosiglitazone&#39; | . &#39;metformin-pioglitazone&#39; STRING | required | | &#39;metformin-pioglitazone&#39; | . &#39;change&#39; STRING | required | | &#39;change&#39; | . &#39;diabetesMed&#39; STRING | required | | &#39;diabetesMed&#39; | . &#39;readmitted&#39; STRING | required | | &#39;readmitted&#39; | . Values . Domain . &#39;race&#39; &#39;AfricanAmerican&#39;, &#39;Asian&#39;, &#39;Caucasian&#39;, &#39;Hispanic&#39;, &#39;Other&#39; | . &#39;gender&#39; &#39;Female&#39;, &#39;Male&#39;, &#39;Unknown/Invalid&#39; | . &#39;age&#39; &#39;[0-10)&#39;, &#39;[10-20)&#39;, &#39;[20-30)&#39;, &#39;[30-40)&#39;, &#39;[40-50)&#39;, &#39;[50-60)&#39;, &#39;[60-70)&#39;, &#39;[70-80)&#39;, &#39;[80-90)&#39;, &#39;[90-100)&#39; | . &#39;weight&#39; &#39;&gt;200&#39;, &#39;[0-25)&#39;, &#39;[100-125)&#39;, &#39;[125-150)&#39;, &#39;[150-175)&#39;, &#39;[175-200)&#39;, &#39;[25-50)&#39;, &#39;[50-75)&#39;, &#39;[75-100)&#39; | . &#39;payer_code&#39; &#39;BC&#39;, &#39;CH&#39;, &#39;CM&#39;, &#39;CP&#39;, &#39;DM&#39;, &#39;HM&#39;, &#39;MC&#39;, &#39;MD&#39;, &#39;MP&#39;, &#39;OG&#39;, &#39;OT&#39;, &#39;PO&#39;, &#39;SI&#39;, &#39;SP&#39;, &#39;UN&#39;, &#39;WC&#39; | . &#39;medical_specialty&#39; &#39;AllergyandImmunology&#39;, &#39;Anesthesiology&#39;, &#39;Anesthesiology-Pediatric&#39;, &#39;Cardiology&#39;, &#39;Cardiology-Pediatric&#39;, &#39;Dentistry&#39;, &#39;Dermatology&#39;, &#39;Emergency/Trauma&#39;, &#39;Endocrinology&#39;, &#39;Family/GeneralPractice&#39;, &#39;Gastroenterology&#39;, &#39;Gynecology&#39;, &#39;Hematology&#39;, &#39;Hematology/Oncology&#39;, &#39;Hospitalist&#39;, &#39;InfectiousDiseases&#39;, &#39;InternalMedicine&#39;, &#39;Nephrology&#39;, &#39;Neurology&#39;, &#39;Obsterics&amp;Gynecology-GynecologicOnco&#39;, &#39;Obstetrics&#39;, &#39;ObstetricsandGynecology&#39;, &#39;Oncology&#39;, &#39;Ophthalmology&#39;, &#39;Orthopedics&#39;, &#39;Orthopedics-Reconstructive&#39;, &#39;Osteopath&#39;, &#39;Otolaryngology&#39;, &#39;OutreachServices&#39;, &#39;Pathology&#39;, &#39;Pediatrics&#39;, &#39;Pediatrics-AllergyandImmunology&#39;, &#39;Pediatrics-CriticalCare&#39;, &#39;Pediatrics-EmergencyMedicine&#39;, &#39;Pediatrics-Endocrinology&#39;, &#39;Pediatrics-Hematology-Oncology&#39;, &#39;Pediatrics-InfectiousDiseases&#39;, &#39;Pediatrics-Neurology&#39;, &#39;Pediatrics-Pulmonology&#39;, &#39;Perinatology&#39;, &#39;PhysicalMedicineandRehabilitation&#39;, &#39;PhysicianNotFound&#39;, &#39;Podiatry&#39;, &#39;Proctology&#39;, &#39;Psychiatry&#39;, &#39;Psychiatry-Addictive&#39;, &#39;Psychiatry-Child/Adolescent&#39;, &#39;Psychology&#39;, &#39;Pulmonology&#39;, &#39;Radiologist&#39;, &#39;Radiology&#39;, &#39;Rheumatology&#39;, &#39;Speech&#39;, &#39;SportsMedicine&#39;, &#39;Surgeon&#39;, &#39;Surgery-Cardiovascular&#39;, &#39;Surgery-Cardiovascular/Thoracic&#39;, &#39;Surgery-Colon&amp;Rectal&#39;, &#39;Surgery-General&#39;, &#39;Surgery-Maxillofacial&#39;, &#39;Surgery-Neuro&#39;, &#39;Surgery-Pediatric&#39;, &#39;Surgery-Plastic&#39;, &#39;Surgery-PlasticwithinHeadandNeck&#39;, &#39;Surgery-Thoracic&#39;, &#39;Surgery-Vascular&#39;, &#39;SurgicalSpecialty&#39;, &#39;Urology&#39;, &#39;Neurophysiology&#39;, &#39;Neurophysiology&#39;, &#39;Neurophysiology&#39;, &#39;Neurophysiology&#39; | . &#39;max_glu_serum&#39; &#39;&gt;200&#39;, &#39;&gt;300&#39;, &#39;None&#39;, &#39;Norm&#39; | . &#39;A1Cresult&#39; &#39;&gt;7&#39;, &#39;&gt;8&#39;, &#39;None&#39;, &#39;Norm&#39; | . &#39;metformin&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;repaglinide&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;nateglinide&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;chlorpropamide&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;glimepiride&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;acetohexamide&#39; &#39;No&#39;, &#39;Steady&#39; | . &#39;glipizide&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;glyburide&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;tolbutamide&#39; &#39;No&#39;, &#39;Steady&#39; | . &#39;pioglitazone&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;rosiglitazone&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;acarbose&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;miglitol&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;troglitazone&#39; &#39;No&#39;, &#39;Steady&#39; | . &#39;tolazamide&#39; &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;examide&#39; &#39;No&#39; | . &#39;citoglipton&#39; &#39;No&#39; | . &#39;insulin&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;glyburide-metformin&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;glipizide-metformin&#39; &#39;No&#39;, &#39;Steady&#39; | . &#39;glimepiride-pioglitazone&#39; &#39;No&#39;, &#39;Steady&#39;, &#39;Steady&#39;, &#39;Steady&#39;, &#39;Steady&#39;, &#39;Steady&#39;, &#39;Steady&#39; | . &#39;metformin-rosiglitazone&#39; &#39;No&#39; | . &#39;metformin-pioglitazone&#39; &#39;No&#39; | . &#39;change&#39; &#39;Ch&#39;, &#39;No&#39; | . &#39;diabetesMed&#39; &#39;No&#39;, &#39;Yes&#39; | . &#39;readmitted&#39; &#39;&lt;30&#39;, &#39;&gt;30&#39;, &#39;NO&#39; | . Towards the bottom of the Domain-Values pairs of the cell above, you can see that many features (including &#39;metformin&#39;) have the same values: [&#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39;]. These values are common to many features including the ones with missing values during schema inference. . TFDV allows you to modify the domains of some features to match an existing domain. To address the detected anomaly, you can set the domain of these features to the domain of the metformin feature. . Complete the function below to set the domain of a feature list to an existing feature domain. . For this, use the tfdv.set_domain() function, which has the following parameters: . schema: The schema | . feature_path: The name of the feature whose domain needs to be set. | . domain: A domain protocol buffer or the name of a global string domain present in the input schema. | . def modify_domain_of_features(features_list, schema, to_domain_name): &#39;&#39;&#39; Modify a list of features&#39; domains. Parameters: features_list : Features that need to be modified schema: Inferred schema to_domain_name : Target domain to be transferred to the features list Returns: schema: new schema &#39;&#39;&#39; ### START CODE HERE # HINT: Loop over the feature list and use set_domain with the inferred schema, feature name and target domain name for feature in features_list: tfdv.set_domain(schema, feature, to_domain_name) ### END CODE HERE return schema . Using this function, set the domain of the features defined in the domain_change_features list below to be equal to metformin&#39;s domain to address the anomalies found. . Since you are overriding the existing domain of the features, it is normal to get a warning so you don&#39;t do this by accident. . domain_change_features = [&#39;repaglinide&#39;, &#39;nateglinide&#39;, &#39;chlorpropamide&#39;, &#39;glimepiride&#39;, &#39;acetohexamide&#39;, &#39;glipizide&#39;, &#39;glyburide&#39;, &#39;tolbutamide&#39;, &#39;pioglitazone&#39;, &#39;rosiglitazone&#39;, &#39;acarbose&#39;, &#39;miglitol&#39;, &#39;troglitazone&#39;, &#39;tolazamide&#39;, &#39;examide&#39;, &#39;citoglipton&#39;, &#39;insulin&#39;, &#39;glyburide-metformin&#39;, &#39;glipizide-metformin&#39;, &#39;glimepiride-pioglitazone&#39;, &#39;metformin-rosiglitazone&#39;, &#39;metformin-pioglitazone&#39;] # Infer new schema by using your modify_domain_of_features function # and the defined domain_change_features feature list schema = modify_domain_of_features(domain_change_features, schema, &#39;metformin&#39;) # Display new schema tfdv.display_schema(schema) . WARNING:root:Replacing existing domain of feature &#34;repaglinide&#34;. WARNING:root:Replacing existing domain of feature &#34;nateglinide&#34;. WARNING:root:Replacing existing domain of feature &#34;chlorpropamide&#34;. WARNING:root:Replacing existing domain of feature &#34;glimepiride&#34;. WARNING:root:Replacing existing domain of feature &#34;acetohexamide&#34;. WARNING:root:Replacing existing domain of feature &#34;glipizide&#34;. WARNING:root:Replacing existing domain of feature &#34;glyburide&#34;. WARNING:root:Replacing existing domain of feature &#34;tolbutamide&#34;. WARNING:root:Replacing existing domain of feature &#34;pioglitazone&#34;. WARNING:root:Replacing existing domain of feature &#34;rosiglitazone&#34;. WARNING:root:Replacing existing domain of feature &#34;acarbose&#34;. WARNING:root:Replacing existing domain of feature &#34;miglitol&#34;. WARNING:root:Replacing existing domain of feature &#34;troglitazone&#34;. WARNING:root:Replacing existing domain of feature &#34;tolazamide&#34;. WARNING:root:Replacing existing domain of feature &#34;examide&#34;. WARNING:root:Replacing existing domain of feature &#34;citoglipton&#34;. WARNING:root:Replacing existing domain of feature &#34;insulin&#34;. WARNING:root:Replacing existing domain of feature &#34;glyburide-metformin&#34;. WARNING:root:Replacing existing domain of feature &#34;glipizide-metformin&#34;. WARNING:root:Replacing existing domain of feature &#34;glimepiride-pioglitazone&#34;. WARNING:root:Replacing existing domain of feature &#34;metformin-rosiglitazone&#34;. WARNING:root:Replacing existing domain of feature &#34;metformin-pioglitazone&#34;. . Type Presence Valency Domain . Feature name . &#39;race&#39; STRING | optional | single | &#39;race&#39; | . &#39;gender&#39; STRING | required | | &#39;gender&#39; | . &#39;age&#39; STRING | required | | &#39;age&#39; | . &#39;weight&#39; STRING | optional | single | &#39;weight&#39; | . &#39;admission_type_id&#39; INT | required | | - | . &#39;discharge_disposition_id&#39; INT | required | | - | . &#39;admission_source_id&#39; INT | required | | - | . &#39;time_in_hospital&#39; INT | required | | - | . &#39;payer_code&#39; STRING | optional | single | &#39;payer_code&#39; | . &#39;medical_specialty&#39; STRING | optional | single | &#39;medical_specialty&#39; | . &#39;num_lab_procedures&#39; INT | required | | - | . &#39;num_procedures&#39; INT | required | | - | . &#39;num_medications&#39; INT | required | | - | . &#39;number_outpatient&#39; INT | required | | - | . &#39;number_emergency&#39; INT | required | | - | . &#39;number_inpatient&#39; INT | required | | - | . &#39;diag_1&#39; BYTES | optional | single | - | . &#39;diag_2&#39; BYTES | optional | single | - | . &#39;diag_3&#39; BYTES | optional | single | - | . &#39;number_diagnoses&#39; INT | required | | - | . &#39;max_glu_serum&#39; STRING | required | | &#39;max_glu_serum&#39; | . &#39;A1Cresult&#39; STRING | required | | &#39;A1Cresult&#39; | . &#39;metformin&#39; STRING | required | | &#39;metformin&#39; | . &#39;repaglinide&#39; STRING | required | | &#39;metformin&#39; | . &#39;nateglinide&#39; STRING | required | | &#39;metformin&#39; | . &#39;chlorpropamide&#39; STRING | required | | &#39;metformin&#39; | . &#39;glimepiride&#39; STRING | required | | &#39;metformin&#39; | . &#39;acetohexamide&#39; STRING | required | | &#39;metformin&#39; | . &#39;glipizide&#39; STRING | required | | &#39;metformin&#39; | . &#39;glyburide&#39; STRING | required | | &#39;metformin&#39; | . &#39;tolbutamide&#39; STRING | required | | &#39;metformin&#39; | . &#39;pioglitazone&#39; STRING | required | | &#39;metformin&#39; | . &#39;rosiglitazone&#39; STRING | required | | &#39;metformin&#39; | . &#39;acarbose&#39; STRING | required | | &#39;metformin&#39; | . &#39;miglitol&#39; STRING | required | | &#39;metformin&#39; | . &#39;troglitazone&#39; STRING | required | | &#39;metformin&#39; | . &#39;tolazamide&#39; STRING | required | | &#39;metformin&#39; | . &#39;examide&#39; STRING | required | | &#39;metformin&#39; | . &#39;citoglipton&#39; STRING | required | | &#39;metformin&#39; | . &#39;insulin&#39; STRING | required | | &#39;metformin&#39; | . &#39;glyburide-metformin&#39; STRING | required | | &#39;metformin&#39; | . &#39;glipizide-metformin&#39; STRING | required | | &#39;metformin&#39; | . &#39;glimepiride-pioglitazone&#39; STRING | required | | &#39;metformin&#39; | . &#39;metformin-rosiglitazone&#39; STRING | required | | &#39;metformin&#39; | . &#39;metformin-pioglitazone&#39; STRING | required | | &#39;metformin&#39; | . &#39;change&#39; STRING | required | | &#39;change&#39; | . &#39;diabetesMed&#39; STRING | required | | &#39;diabetesMed&#39; | . &#39;readmitted&#39; STRING | required | | &#39;readmitted&#39; | . Values . Domain . &#39;race&#39; &#39;AfricanAmerican&#39;, &#39;Asian&#39;, &#39;Caucasian&#39;, &#39;Hispanic&#39;, &#39;Other&#39; | . &#39;gender&#39; &#39;Female&#39;, &#39;Male&#39;, &#39;Unknown/Invalid&#39; | . &#39;age&#39; &#39;[0-10)&#39;, &#39;[10-20)&#39;, &#39;[20-30)&#39;, &#39;[30-40)&#39;, &#39;[40-50)&#39;, &#39;[50-60)&#39;, &#39;[60-70)&#39;, &#39;[70-80)&#39;, &#39;[80-90)&#39;, &#39;[90-100)&#39; | . &#39;weight&#39; &#39;&gt;200&#39;, &#39;[0-25)&#39;, &#39;[100-125)&#39;, &#39;[125-150)&#39;, &#39;[150-175)&#39;, &#39;[175-200)&#39;, &#39;[25-50)&#39;, &#39;[50-75)&#39;, &#39;[75-100)&#39; | . &#39;payer_code&#39; &#39;BC&#39;, &#39;CH&#39;, &#39;CM&#39;, &#39;CP&#39;, &#39;DM&#39;, &#39;HM&#39;, &#39;MC&#39;, &#39;MD&#39;, &#39;MP&#39;, &#39;OG&#39;, &#39;OT&#39;, &#39;PO&#39;, &#39;SI&#39;, &#39;SP&#39;, &#39;UN&#39;, &#39;WC&#39; | . &#39;medical_specialty&#39; &#39;AllergyandImmunology&#39;, &#39;Anesthesiology&#39;, &#39;Anesthesiology-Pediatric&#39;, &#39;Cardiology&#39;, &#39;Cardiology-Pediatric&#39;, &#39;Dentistry&#39;, &#39;Dermatology&#39;, &#39;Emergency/Trauma&#39;, &#39;Endocrinology&#39;, &#39;Family/GeneralPractice&#39;, &#39;Gastroenterology&#39;, &#39;Gynecology&#39;, &#39;Hematology&#39;, &#39;Hematology/Oncology&#39;, &#39;Hospitalist&#39;, &#39;InfectiousDiseases&#39;, &#39;InternalMedicine&#39;, &#39;Nephrology&#39;, &#39;Neurology&#39;, &#39;Obsterics&amp;Gynecology-GynecologicOnco&#39;, &#39;Obstetrics&#39;, &#39;ObstetricsandGynecology&#39;, &#39;Oncology&#39;, &#39;Ophthalmology&#39;, &#39;Orthopedics&#39;, &#39;Orthopedics-Reconstructive&#39;, &#39;Osteopath&#39;, &#39;Otolaryngology&#39;, &#39;OutreachServices&#39;, &#39;Pathology&#39;, &#39;Pediatrics&#39;, &#39;Pediatrics-AllergyandImmunology&#39;, &#39;Pediatrics-CriticalCare&#39;, &#39;Pediatrics-EmergencyMedicine&#39;, &#39;Pediatrics-Endocrinology&#39;, &#39;Pediatrics-Hematology-Oncology&#39;, &#39;Pediatrics-InfectiousDiseases&#39;, &#39;Pediatrics-Neurology&#39;, &#39;Pediatrics-Pulmonology&#39;, &#39;Perinatology&#39;, &#39;PhysicalMedicineandRehabilitation&#39;, &#39;PhysicianNotFound&#39;, &#39;Podiatry&#39;, &#39;Proctology&#39;, &#39;Psychiatry&#39;, &#39;Psychiatry-Addictive&#39;, &#39;Psychiatry-Child/Adolescent&#39;, &#39;Psychology&#39;, &#39;Pulmonology&#39;, &#39;Radiologist&#39;, &#39;Radiology&#39;, &#39;Rheumatology&#39;, &#39;Speech&#39;, &#39;SportsMedicine&#39;, &#39;Surgeon&#39;, &#39;Surgery-Cardiovascular&#39;, &#39;Surgery-Cardiovascular/Thoracic&#39;, &#39;Surgery-Colon&amp;Rectal&#39;, &#39;Surgery-General&#39;, &#39;Surgery-Maxillofacial&#39;, &#39;Surgery-Neuro&#39;, &#39;Surgery-Pediatric&#39;, &#39;Surgery-Plastic&#39;, &#39;Surgery-PlasticwithinHeadandNeck&#39;, &#39;Surgery-Thoracic&#39;, &#39;Surgery-Vascular&#39;, &#39;SurgicalSpecialty&#39;, &#39;Urology&#39;, &#39;Neurophysiology&#39;, &#39;Neurophysiology&#39;, &#39;Neurophysiology&#39;, &#39;Neurophysiology&#39; | . &#39;max_glu_serum&#39; &#39;&gt;200&#39;, &#39;&gt;300&#39;, &#39;None&#39;, &#39;Norm&#39; | . &#39;A1Cresult&#39; &#39;&gt;7&#39;, &#39;&gt;8&#39;, &#39;None&#39;, &#39;Norm&#39; | . &#39;metformin&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;repaglinide&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;nateglinide&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;chlorpropamide&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;glimepiride&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;acetohexamide&#39; &#39;No&#39;, &#39;Steady&#39; | . &#39;glipizide&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;glyburide&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;tolbutamide&#39; &#39;No&#39;, &#39;Steady&#39; | . &#39;pioglitazone&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;rosiglitazone&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;acarbose&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;miglitol&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;troglitazone&#39; &#39;No&#39;, &#39;Steady&#39; | . &#39;tolazamide&#39; &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;examide&#39; &#39;No&#39; | . &#39;citoglipton&#39; &#39;No&#39; | . &#39;insulin&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;glyburide-metformin&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;glipizide-metformin&#39; &#39;No&#39;, &#39;Steady&#39; | . &#39;glimepiride-pioglitazone&#39; &#39;No&#39;, &#39;Steady&#39;, &#39;Steady&#39;, &#39;Steady&#39;, &#39;Steady&#39;, &#39;Steady&#39;, &#39;Steady&#39; | . &#39;metformin-rosiglitazone&#39; &#39;No&#39; | . &#39;metformin-pioglitazone&#39; &#39;No&#39; | . &#39;change&#39; &#39;Ch&#39;, &#39;No&#39; | . &#39;diabetesMed&#39; &#39;No&#39;, &#39;Yes&#39; | . &#39;readmitted&#39; &#39;&lt;30&#39;, &#39;&gt;30&#39;, &#39;NO&#39; | . # check that the domain of some features are now switched to `metformin` print(f&quot;Domain name of &#39;chlorpropamide&#39;: {tfdv.get_feature(schema, &#39;chlorpropamide&#39;).domain}&quot;) print(f&quot;Domain values of &#39;chlorpropamide&#39;: {tfdv.get_domain(schema, &#39;chlorpropamide&#39;).value}&quot;) print(f&quot;Domain name of &#39;repaglinide&#39;: {tfdv.get_feature(schema, &#39;repaglinide&#39;).domain}&quot;) print(f&quot;Domain values of &#39;repaglinide&#39;: {tfdv.get_domain(schema, &#39;repaglinide&#39;).value}&quot;) print(f&quot;Domain name of &#39;nateglinide&#39;: {tfdv.get_feature(schema, &#39;nateglinide&#39;).domain}&quot;) print(f&quot;Domain values of &#39;nateglinide&#39;: {tfdv.get_domain(schema, &#39;nateglinide&#39;).value}&quot;) . Domain name of &#39;chlorpropamide&#39;: metformin Domain values of &#39;chlorpropamide&#39;: [&#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39;] Domain name of &#39;repaglinide&#39;: metformin Domain values of &#39;repaglinide&#39;: [&#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39;] Domain name of &#39;nateglinide&#39;: metformin Domain values of &#39;nateglinide&#39;: [&#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39;] . Expected Output: . Domain name of &#39;chlorpropamide&#39;: metformin Domain values of &#39;chlorpropamide&#39;: [&#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39;] Domain name of &#39;repaglinide&#39;: metformin Domain values of &#39;repaglinide&#39;: [&#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39;] Domain name of &#39;nateglinide&#39;: metformin Domain values of &#39;nateglinide&#39;: [&#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39;] . Let&#39;s do a final check of anomalies to see if this solved the issue. . calculate_and_display_anomalies(serving_stats, schema=schema) . Anomaly short description Anomaly long description . Feature name . &#39;readmitted&#39; Column dropped | Column is completely missing | . You should now see the metformin-pioglitazone and metformin-rosiglitazone features dropped from the output anomalies. . . Exercise 9: Detecting anomalies with environments . There is still one thing to address. The readmitted feature (which is the label column) showed up as an anomaly (&#39;Column dropped&#39;). Since labels are not expected in the serving data, let&#39;s tell TFDV to ignore this detected anomaly. . This requirement of introducing slight schema variations can be expressed by using environments. In particular, features in the schema can be associated with a set of environments using default_environment, in_environment and not_in_environment. . schema.default_environment.append(&#39;TRAINING&#39;) schema.default_environment.append(&#39;SERVING&#39;) tfdv.display_schema(schema) . Type Presence Valency Domain . Feature name . &#39;race&#39; STRING | optional | single | &#39;race&#39; | . &#39;gender&#39; STRING | required | | &#39;gender&#39; | . &#39;age&#39; STRING | required | | &#39;age&#39; | . &#39;weight&#39; STRING | optional | single | &#39;weight&#39; | . &#39;admission_type_id&#39; INT | required | | - | . &#39;discharge_disposition_id&#39; INT | required | | - | . &#39;admission_source_id&#39; INT | required | | - | . &#39;time_in_hospital&#39; INT | required | | - | . &#39;payer_code&#39; STRING | optional | single | &#39;payer_code&#39; | . &#39;medical_specialty&#39; STRING | optional | single | &#39;medical_specialty&#39; | . &#39;num_lab_procedures&#39; INT | required | | - | . &#39;num_procedures&#39; INT | required | | - | . &#39;num_medications&#39; INT | required | | - | . &#39;number_outpatient&#39; INT | required | | - | . &#39;number_emergency&#39; INT | required | | - | . &#39;number_inpatient&#39; INT | required | | - | . &#39;diag_1&#39; BYTES | optional | single | - | . &#39;diag_2&#39; BYTES | optional | single | - | . &#39;diag_3&#39; BYTES | optional | single | - | . &#39;number_diagnoses&#39; INT | required | | - | . &#39;max_glu_serum&#39; STRING | required | | &#39;max_glu_serum&#39; | . &#39;A1Cresult&#39; STRING | required | | &#39;A1Cresult&#39; | . &#39;metformin&#39; STRING | required | | &#39;metformin&#39; | . &#39;repaglinide&#39; STRING | required | | &#39;metformin&#39; | . &#39;nateglinide&#39; STRING | required | | &#39;metformin&#39; | . &#39;chlorpropamide&#39; STRING | required | | &#39;metformin&#39; | . &#39;glimepiride&#39; STRING | required | | &#39;metformin&#39; | . &#39;acetohexamide&#39; STRING | required | | &#39;metformin&#39; | . &#39;glipizide&#39; STRING | required | | &#39;metformin&#39; | . &#39;glyburide&#39; STRING | required | | &#39;metformin&#39; | . &#39;tolbutamide&#39; STRING | required | | &#39;metformin&#39; | . &#39;pioglitazone&#39; STRING | required | | &#39;metformin&#39; | . &#39;rosiglitazone&#39; STRING | required | | &#39;metformin&#39; | . &#39;acarbose&#39; STRING | required | | &#39;metformin&#39; | . &#39;miglitol&#39; STRING | required | | &#39;metformin&#39; | . &#39;troglitazone&#39; STRING | required | | &#39;metformin&#39; | . &#39;tolazamide&#39; STRING | required | | &#39;metformin&#39; | . &#39;examide&#39; STRING | required | | &#39;metformin&#39; | . &#39;citoglipton&#39; STRING | required | | &#39;metformin&#39; | . &#39;insulin&#39; STRING | required | | &#39;metformin&#39; | . &#39;glyburide-metformin&#39; STRING | required | | &#39;metformin&#39; | . &#39;glipizide-metformin&#39; STRING | required | | &#39;metformin&#39; | . &#39;glimepiride-pioglitazone&#39; STRING | required | | &#39;metformin&#39; | . &#39;metformin-rosiglitazone&#39; STRING | required | | &#39;metformin&#39; | . &#39;metformin-pioglitazone&#39; STRING | required | | &#39;metformin&#39; | . &#39;change&#39; STRING | required | | &#39;change&#39; | . &#39;diabetesMed&#39; STRING | required | | &#39;diabetesMed&#39; | . &#39;readmitted&#39; STRING | required | | &#39;readmitted&#39; | . Values . Domain . &#39;race&#39; &#39;AfricanAmerican&#39;, &#39;Asian&#39;, &#39;Caucasian&#39;, &#39;Hispanic&#39;, &#39;Other&#39; | . &#39;gender&#39; &#39;Female&#39;, &#39;Male&#39;, &#39;Unknown/Invalid&#39; | . &#39;age&#39; &#39;[0-10)&#39;, &#39;[10-20)&#39;, &#39;[20-30)&#39;, &#39;[30-40)&#39;, &#39;[40-50)&#39;, &#39;[50-60)&#39;, &#39;[60-70)&#39;, &#39;[70-80)&#39;, &#39;[80-90)&#39;, &#39;[90-100)&#39; | . &#39;weight&#39; &#39;&gt;200&#39;, &#39;[0-25)&#39;, &#39;[100-125)&#39;, &#39;[125-150)&#39;, &#39;[150-175)&#39;, &#39;[175-200)&#39;, &#39;[25-50)&#39;, &#39;[50-75)&#39;, &#39;[75-100)&#39; | . &#39;payer_code&#39; &#39;BC&#39;, &#39;CH&#39;, &#39;CM&#39;, &#39;CP&#39;, &#39;DM&#39;, &#39;HM&#39;, &#39;MC&#39;, &#39;MD&#39;, &#39;MP&#39;, &#39;OG&#39;, &#39;OT&#39;, &#39;PO&#39;, &#39;SI&#39;, &#39;SP&#39;, &#39;UN&#39;, &#39;WC&#39; | . &#39;medical_specialty&#39; &#39;AllergyandImmunology&#39;, &#39;Anesthesiology&#39;, &#39;Anesthesiology-Pediatric&#39;, &#39;Cardiology&#39;, &#39;Cardiology-Pediatric&#39;, &#39;Dentistry&#39;, &#39;Dermatology&#39;, &#39;Emergency/Trauma&#39;, &#39;Endocrinology&#39;, &#39;Family/GeneralPractice&#39;, &#39;Gastroenterology&#39;, &#39;Gynecology&#39;, &#39;Hematology&#39;, &#39;Hematology/Oncology&#39;, &#39;Hospitalist&#39;, &#39;InfectiousDiseases&#39;, &#39;InternalMedicine&#39;, &#39;Nephrology&#39;, &#39;Neurology&#39;, &#39;Obsterics&amp;Gynecology-GynecologicOnco&#39;, &#39;Obstetrics&#39;, &#39;ObstetricsandGynecology&#39;, &#39;Oncology&#39;, &#39;Ophthalmology&#39;, &#39;Orthopedics&#39;, &#39;Orthopedics-Reconstructive&#39;, &#39;Osteopath&#39;, &#39;Otolaryngology&#39;, &#39;OutreachServices&#39;, &#39;Pathology&#39;, &#39;Pediatrics&#39;, &#39;Pediatrics-AllergyandImmunology&#39;, &#39;Pediatrics-CriticalCare&#39;, &#39;Pediatrics-EmergencyMedicine&#39;, &#39;Pediatrics-Endocrinology&#39;, &#39;Pediatrics-Hematology-Oncology&#39;, &#39;Pediatrics-InfectiousDiseases&#39;, &#39;Pediatrics-Neurology&#39;, &#39;Pediatrics-Pulmonology&#39;, &#39;Perinatology&#39;, &#39;PhysicalMedicineandRehabilitation&#39;, &#39;PhysicianNotFound&#39;, &#39;Podiatry&#39;, &#39;Proctology&#39;, &#39;Psychiatry&#39;, &#39;Psychiatry-Addictive&#39;, &#39;Psychiatry-Child/Adolescent&#39;, &#39;Psychology&#39;, &#39;Pulmonology&#39;, &#39;Radiologist&#39;, &#39;Radiology&#39;, &#39;Rheumatology&#39;, &#39;Speech&#39;, &#39;SportsMedicine&#39;, &#39;Surgeon&#39;, &#39;Surgery-Cardiovascular&#39;, &#39;Surgery-Cardiovascular/Thoracic&#39;, &#39;Surgery-Colon&amp;Rectal&#39;, &#39;Surgery-General&#39;, &#39;Surgery-Maxillofacial&#39;, &#39;Surgery-Neuro&#39;, &#39;Surgery-Pediatric&#39;, &#39;Surgery-Plastic&#39;, &#39;Surgery-PlasticwithinHeadandNeck&#39;, &#39;Surgery-Thoracic&#39;, &#39;Surgery-Vascular&#39;, &#39;SurgicalSpecialty&#39;, &#39;Urology&#39;, &#39;Neurophysiology&#39;, &#39;Neurophysiology&#39;, &#39;Neurophysiology&#39;, &#39;Neurophysiology&#39; | . &#39;max_glu_serum&#39; &#39;&gt;200&#39;, &#39;&gt;300&#39;, &#39;None&#39;, &#39;Norm&#39; | . &#39;A1Cresult&#39; &#39;&gt;7&#39;, &#39;&gt;8&#39;, &#39;None&#39;, &#39;Norm&#39; | . &#39;metformin&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;repaglinide&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;nateglinide&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;chlorpropamide&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;glimepiride&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;acetohexamide&#39; &#39;No&#39;, &#39;Steady&#39; | . &#39;glipizide&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;glyburide&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;tolbutamide&#39; &#39;No&#39;, &#39;Steady&#39; | . &#39;pioglitazone&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;rosiglitazone&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;acarbose&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;miglitol&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;troglitazone&#39; &#39;No&#39;, &#39;Steady&#39; | . &#39;tolazamide&#39; &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;examide&#39; &#39;No&#39; | . &#39;citoglipton&#39; &#39;No&#39; | . &#39;insulin&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;glyburide-metformin&#39; &#39;Down&#39;, &#39;No&#39;, &#39;Steady&#39;, &#39;Up&#39; | . &#39;glipizide-metformin&#39; &#39;No&#39;, &#39;Steady&#39; | . &#39;glimepiride-pioglitazone&#39; &#39;No&#39;, &#39;Steady&#39;, &#39;Steady&#39;, &#39;Steady&#39;, &#39;Steady&#39;, &#39;Steady&#39;, &#39;Steady&#39; | . &#39;metformin-rosiglitazone&#39; &#39;No&#39; | . &#39;metformin-pioglitazone&#39; &#39;No&#39; | . &#39;change&#39; &#39;Ch&#39;, &#39;No&#39; | . &#39;diabetesMed&#39; &#39;No&#39;, &#39;Yes&#39; | . &#39;readmitted&#39; &#39;&lt;30&#39;, &#39;&gt;30&#39;, &#39;NO&#39; | . Complete the code below to exclude the readmitted feature from the SERVING environment. . To achieve this, you can use the tfdv.get_feature() function to get the readmitted feature from the inferred schema and use its not_in_environment attribute to specify that readmitted should be removed from the SERVING environment&#39;s schema. This attribute is a list so you will have to append the name of the environment that you wish to omit this feature for. . To be more explicit, given a feature you can do something like: . feature.not_in_environment.append(&#39;NAME_OF_ENVIRONMENT&#39;) . The function tfdv.get_feature receives the following parameters: . schema: The schema. | feature_path: The path of the feature to obtain from the schema. In this case this is equal to the name of the feature. | . # Specify that &#39;readmitted&#39; feature is not in SERVING environment. # HINT: Append the &#39;SERVING&#39; environmnet to the not_in_environment attribute of the feature #tfdv.get_feature(schema, None).not_in_environment.append(None) not_in_environment_feature = tfdv.get_feature(schema,&#39;readmitted&#39;).not_in_environment.append(&#39;SERVING&#39;) # HINT: Calculate anomalies with the validate_statistics function by using the serving statistics, # inferred schema and the SERVING environment parameter. #serving_anomalies_with_env = tfdv.validate_statistics(None, schema, environment=None) serving_anomalies_with_env = tfdv.validate_statistics(serving_stats, schema, environment=not_in_environment_feature) ### END CODE HERE . You should see &quot;No anomalies found&quot; by running the cell below. . tfdv.display_anomalies(serving_anomalies_with_env) . Anomaly short description Anomaly long description . Feature name . &#39;readmitted&#39; Column dropped | Column is completely missing | . Now you have succesfully addressed all anomaly-related issues! . . 7 - Check for Data Drift and Skew . During data validation, you also need to check for data drift and data skew between the training and serving data. You can do this by specifying the skew_comparator and drift_comparator in the schema. . Drift and skew is expressed in terms of L-infinity distance which evaluates the difference between vectors as the greatest of the differences along any coordinate dimension. . You can set the threshold distance so that you receive warnings when the drift is higher than is acceptable. Setting the correct distance is typically an iterative process requiring domain knowledge and experimentation. . Let&#39;s check for the skew in the diabetesMed feature and drift in the payer_code feature. . diabetes_med = tfdv.get_feature(schema, &#39;diabetesMed&#39;) diabetes_med.skew_comparator.infinity_norm.threshold = 0.035 # domain knowledge helps to determine this threshold # Calculate drift for the payer_code feature payer_code = tfdv.get_feature(schema, &#39;payer_code&#39;) payer_code.drift_comparator.infinity_norm.threshold = 0.035 # domain knowledge helps to determine this threshold # Calculate anomalies skew_drift_anomalies = tfdv.validate_statistics(train_stats, schema, previous_statistics=eval_stats, serving_statistics=serving_stats) # Display anomalies tfdv.display_anomalies(skew_drift_anomalies) . No anomalies found. . In both of these cases, the detected anomaly distance is not too far from the threshold value of 0.03. For this exercise, let&#39;s accept this as within bounds (i.e. you can set the distance to something like 0.035 instead). . However, if the anomaly truly indicates a skew and drift, then further investigation is necessary as this could have a direct impact on model performance. . . 8 - Display Stats for Data Slices . Finally, you can slice the dataset and calculate the statistics for each unique value of a feature. By default, TFDV computes statistics for the overall dataset in addition to the configured slices. Each slice is identified by a unique name which is set as the dataset name in the DatasetFeatureStatistics protocol buffer. Generating and displaying statistics over different slices of data can help track model and anomaly metrics. . Let&#39;s first define a few helper functions to make our code in the exercise more neat. . def split_datasets(dataset_list): &#39;&#39;&#39; split datasets. Parameters: dataset_list: List of datasets to split Returns: datasets: sliced data &#39;&#39;&#39; datasets = [] for dataset in dataset_list.datasets: proto_list = DatasetFeatureStatisticsList() proto_list.datasets.extend([dataset]) datasets.append(proto_list) return datasets def display_stats_at_index(index, datasets): &#39;&#39;&#39; display statistics at the specified data index Parameters: index : index to show the anomalies datasets: split data Returns: display of generated sliced data statistics at the specified index &#39;&#39;&#39; if index &lt; len(datasets): print(datasets[index].datasets[0].name) tfdv.visualize_statistics(datasets[index]) . The function below returns a list of DatasetFeatureStatisticsList protocol buffers. As shown in the ungraded lab, the first one will be for All Examples followed by individual slices through the feature you specified. . To configure TFDV to generate statistics for dataset slices, you will use the function tfdv.StatsOptions() with the following 4 arguments: . schema | . slice_functions passed as a list. | . infer_type_from_schema set to True. | . feature_whitelist set to the approved features. | . Remember that slice_functions only work with generate_statistics_from_csv() so you will need to convert the dataframe to CSV. . def sliced_stats_for_slice_fn(slice_fn, approved_cols, dataframe, schema): &#39;&#39;&#39; generate statistics for the sliced data. Parameters: slice_fn : slicing definition approved_cols: list of features to pass to the statistics options dataframe: pandas dataframe to slice schema: the schema Returns: slice_info_datasets: statistics for the sliced dataset &#39;&#39;&#39; # Set the StatsOptions slice_stats_options = tfdv.StatsOptions(schema=schema, slice_functions=[slice_fn], infer_type_from_schema=True, feature_whitelist=approved_cols) # Convert Dataframe to CSV since `slice_functions` works only with `tfdv.generate_statistics_from_csv` CSV_PATH = &#39;slice_sample.csv&#39; dataframe.to_csv(CSV_PATH) # Calculate statistics for the sliced dataset sliced_stats = tfdv.generate_statistics_from_csv(CSV_PATH, stats_options=slice_stats_options) # Split the dataset using the previously defined split_datasets function slice_info_datasets = split_datasets(sliced_stats) return slice_info_datasets . With that, you can now use the helper functions to generate and visualize statistics for the sliced datasets. . slice_fn = slicing_util.get_feature_value_slicer(features={&#39;medical_specialty&#39;: None}) # Generate stats for the sliced dataset slice_datasets = sliced_stats_for_slice_fn(slice_fn, approved_cols, dataframe=train_df, schema=schema) # Print name of slices for reference print(f&#39;Statistics generated for: n&#39;) print(&#39; n&#39;.join([sliced.datasets[0].name for sliced in slice_datasets])) # Display at index 10, which corresponds to the slice named `medical_specialty_Gastroenterology` display_stats_at_index(1, slice_datasets) . Statistics generated for: All Examples medical_specialty_Orthopedics medical_specialty_InternalMedicine medical_specialty_Cardiology medical_specialty_Family/GeneralPractice medical_specialty_Surgery-General medical_specialty_Emergency/Trauma medical_specialty_Nephrology medical_specialty_Surgery-Neuro medical_specialty_Oncology medical_specialty_Gastroenterology medical_specialty_Orthopedics-Reconstructive medical_specialty_ObstetricsandGynecology medical_specialty_Surgery-Cardiovascular/Thoracic medical_specialty_Radiologist medical_specialty_Urology medical_specialty_Surgery-Vascular medical_specialty_Hematology/Oncology medical_specialty_Neurology medical_specialty_Psychology medical_specialty_Psychiatry medical_specialty_PhysicalMedicineandRehabilitation medical_specialty_Pulmonology medical_specialty_Otolaryngology medical_specialty_Obsterics&amp;Gynecology-GynecologicOnco medical_specialty_Endocrinology medical_specialty_Anesthesiology medical_specialty_Pediatrics-Endocrinology medical_specialty_Radiology medical_specialty_Pediatrics medical_specialty_Pediatrics-Pulmonology medical_specialty_Osteopath medical_specialty_Surgery-Plastic medical_specialty_Podiatry medical_specialty_Surgery-Thoracic medical_specialty_Rheumatology medical_specialty_Obstetrics medical_specialty_Pediatrics-AllergyandImmunology medical_specialty_Surgery-Cardiovascular medical_specialty_Anesthesiology-Pediatric medical_specialty_Pathology medical_specialty_Pediatrics-CriticalCare medical_specialty_PhysicianNotFound medical_specialty_Gynecology medical_specialty_AllergyandImmunology medical_specialty_Surgery-Maxillofacial medical_specialty_Hospitalist medical_specialty_Hematology medical_specialty_Surgeon medical_specialty_Proctology medical_specialty_InfectiousDiseases medical_specialty_Psychiatry-Child/Adolescent medical_specialty_SurgicalSpecialty medical_specialty_Ophthalmology medical_specialty_Surgery-Pediatric medical_specialty_Pediatrics-Neurology medical_specialty_Surgery-PlasticwithinHeadandNeck medical_specialty_OutreachServices medical_specialty_Pediatrics-Hematology-Oncology medical_specialty_Dentistry medical_specialty_Pediatrics-EmergencyMedicine medical_specialty_Psychiatry-Addictive medical_specialty_Surgery-Colon&amp;Rectal medical_specialty_Pediatrics-InfectiousDiseases medical_specialty_Dermatology medical_specialty_Perinatology medical_specialty_SportsMedicine medical_specialty_Cardiology-Pediatric medical_specialty_Speech medical_specialty_Orthopedics . If you are curious, try different slice indices to extract the group statistics. For instance, index=5 corresponds to all medical_specialty_Surgery-General records. You can also try slicing through multiple features as shown in the ungraded lab. . Another challenge is to implement your own helper functions. For instance, you can make a display_stats_for_slice_name() function so you don&#39;t have to determine the index of a slice. If done correctly, you can just do display_stats_for_slice_name(&#39;medical_specialty_Gastroenterology&#39;, slice_datasets) and it will generate the same result as display_stats_at_index(10, slice_datasets). . . 9 - Freeze the schema . Now that the schema has been reviewed, you will store the schema in a file in its &quot;frozen&quot; state. This can be used to validate incoming data once your application goes live to your users. . This is pretty straightforward using Tensorflow&#39;s io utils and TFDV&#39;s write_schema_text() function. . OUTPUT_DIR = &quot;output&quot; file_io.recursive_create_dir(OUTPUT_DIR) # Use TensorFlow text output format pbtxt to store the schema schema_file = os.path.join(OUTPUT_DIR, &#39;schema.pbtxt&#39;) # write_schema_text function expect the defined schema and output path as parameters tfdv.write_schema_text(schema, schema_file) . After submitting this assignment, you can click the Jupyter logo in the left upper corner of the screen to check the Jupyter filesystem. The schema.pbtxt file should be inside the output directory. . Congratulations on finishing this week&#39;s assignment! A lot of concepts where introduced and now you should feel more familiar with using TFDV for inferring schemas, anomaly detection and other data-related tasks. . Keep it up! .",
            "url": "https://marcelcastrobr.github.io/notebooks/jupyter/tensorflow/2021/09/04/FeatureEngTensorflow.html",
            "relUrl": "/jupyter/tensorflow/2021/09/04/FeatureEngTensorflow.html",
            "date": " • Sep 4, 2021"
        }
        
    
  
    
        ,"post9": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://marcelcastrobr.github.io/notebooks/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": ". - . marcelcastrobr@gmail.com . I made this website to capture my learning in the field of artificial intelligence, machine learning, data science and devops. Check post and notebooks. . I am Marcel Cavalcanti de Castro, born in Brazil and working as a data scientist in Norway. . 2004: start working as a network engineer in Brazil working with embedded linux and wireless ad-hoc/mesh network to provide internet access to rural areas. | 2006: decided to move to Karlstad, Sweden to pursue my Ph.D. in computer science on wireless mesh networks. A lot of fun working with mathematical models, simulation and testbed experiment to check reality. :smile: . | 2010: got my first :baby:. Love the experience. :baby_symbol: | 2011: second and last :baby:. Family completed. :family_man_woman_girl_boy: | 2012: moved to Kongsberg, Norway to work in the energy sector. | . Last update at - 2021-11-02T08:07:00Z .",
          "url": "https://marcelcastrobr.github.io/notebooks/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://marcelcastrobr.github.io/notebooks/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}